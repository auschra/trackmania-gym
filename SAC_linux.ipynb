{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gettelemetry as client\n",
    "import gamepad as gp\n",
    "import window as gwd\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import mss\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import pywinctl as gw\n",
    "import vgamepad as vg\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mauschra3\u001b[0m (\u001b[33mauschra3-massachusetts-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/auschra/ml/trackmania/trackmaniarl/wandb/run-20241119_123757-4iltf3cy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/4iltf3cy' target=\"_blank\">misty-morning-58</a></strong> to <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac' target=\"_blank\">https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/4iltf3cy' target=\"_blank\">https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/4iltf3cy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/4iltf3cy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79ffee4e2270>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"trackmania_sac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowCap():\n",
    "    def __init__(self, window_name):\n",
    "        self.window_name = window_name\n",
    "        self.window = gw.getWindowsWithTitle(window_name)\n",
    "        if not self.window:\n",
    "            raise Exception(f\"Window with name '{window_name}' not found.\")\n",
    "        self.window = self.window[0]\n",
    "        self.top = self.window.top\n",
    "        self.left = self.window.left\n",
    "        self.width = self.window.width\n",
    "        self.height = self.window.height\n",
    "        self.monitor = {\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height}\n",
    "        self.sct = mss.mss()\n",
    "        self.resize = 128\n",
    "\n",
    "    def capture(self):\n",
    "        img = np.array(self.sct.grab(self.monitor))\n",
    "        img = cv2.resize(img, (self.resize, self.resize))       # resize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)        # Convert to grayscale\n",
    "        img = img / 255.0           # normalize\n",
    "        return img\n",
    "\n",
    "    def __del__(self):\n",
    "        self.sct.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load optimal positons \n",
    "with open('models/optimal_positons.txt', 'r') as f:\n",
    "    optimal_positions = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackmaniaEnv(gym.Env) :\n",
    "    def __init__(self, window_name=\"Trackmania\"):\n",
    "        super(TrackmaniaEnv, self).__init__()\n",
    "        self.window = WindowCap(window_name)\n",
    "        self.client = client.TMClient()\n",
    "        self.gamepad = gp.GamepadHandler()\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=np.array([-1.0, 0.0, 0.0]),\n",
    "            high=np.array([1.0, 1.0, 1.0]), \n",
    "            dtype=np.float32)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(1, 128, 128), dtype=np.uint8)\n",
    "        self.reward_range = (-np.inf, np.inf)\n",
    "        self.metadata = {'render.modes': ['human']}\n",
    "        self.spec = None\n",
    "        self.terminated = False\n",
    "        self.truncated = False\n",
    "        self.reward = 0\n",
    "        self.prev_action = np.array([0, 0, 0])\n",
    "        self.prev_obs = np.zeros((1, 128, 128), dtype=np.uint8)\n",
    "        \n",
    "        # set the window id and focus\n",
    "        self.id = gwd.get_window_id(\"Trackmania\")      \n",
    "        self.focus = gwd.focus_window(self.id)\n",
    "        self.speed_buffer = []\n",
    "\n",
    "        self.steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.time_start = 0\n",
    "        self.telemetry = self.client.retrieve_data()\n",
    "        self.optimal_positions = optimal_positions\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        self.gamepad.reset()\n",
    "        self.speed_buffer = []\n",
    "        time.sleep(1.5)\n",
    "        #self.focus = gwd.focus_window(self.id)\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.terminated = False\n",
    "        self.reward = 0\n",
    "        self.prev_action = np.array([0, 0, 0])\n",
    "        obs = self.window.capture()\n",
    "        obs = np.expand_dims(obs, axis=0)  # add channel dim for gym\n",
    "        obs = obs.astype(np.float32)\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "        # Rreturn reset as per gym format\n",
    "        return obs, {}\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        # send action and set the new previous actions\n",
    "        self.gamepad.send_action(action)\n",
    "        self.prev_action = action\n",
    "            \n",
    "        # capture img obs and retrieve telemetry data\n",
    "        obs = self.window.capture()\n",
    "        self.telemetry = self.client.retrieve_data() \n",
    "\n",
    "        # telemetry data\n",
    "        checkpoint = self.telemetry['checkpoint']\n",
    "        lap = self.telemetry['lap']\n",
    "        speed = self.telemetry['speed']\n",
    "        position = self.telemetry['position']\n",
    "        steer = self.telemetry['steer']\n",
    "        gas = self.telemetry['gas']\n",
    "        brake = self.telemetry['brake']\n",
    "        finished = self.telemetry['finished']\n",
    "        acceleration = self.telemetry['acceleration']\n",
    "        jerk = self.telemetry['jerk']\n",
    "        aim_yaw = self.telemetry['aim_yaw']\n",
    "        aim_pitch = self.telemetry['aim_pitch']\n",
    "        fl_steer_angle = self.telemetry['fl_steer_angle']\n",
    "        fr_steer_angle = self.telemetry['fr_steer_angle']\n",
    "        fl_slip = self.telemetry['fl_slip']\n",
    "        fr_slip = self.telemetry['fr_slip']\n",
    "        gear = self.telemetry['gear']\n",
    "\n",
    "        # call reward function\n",
    "        reward = self.get_reward(self.telemetry)\n",
    "\n",
    "        # determine if crashed by checking window of speed\n",
    "        # TODO -> change to check if positions are not chaning much\n",
    "        self.speed_buffer.append(speed)\n",
    "        if len(self.speed_buffer) > 50:\n",
    "            self.speed_buffer.pop(0)\n",
    "        speed_av = sum(self.speed_buffer) / len(self.speed_buffer)\n",
    "\n",
    "        if speed_av < 2 and acceleration <0.1 and time.time() - self.time_start > 5:\n",
    "            self.terminated = True\n",
    "\n",
    "        # check for complete track and reset if so\n",
    "        if finished:\n",
    "            self.gamepad.press_a()\n",
    "            self.terminated = True\n",
    "        \n",
    "        # log data\n",
    "        if self.steps % 100 == 0:\n",
    "            print(f\"step: {self.steps} / {steps}\")\n",
    "        self.reward = reward\n",
    "        truncated = False\n",
    "        terminated = self.terminated\n",
    "        info = {\n",
    "            'speed': speed,\n",
    "            'position': position,\n",
    "            'checkpoint': checkpoint,\n",
    "            'lap': lap,\n",
    "        }\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def get_reward(self, telemetry, optimal_positions):\n",
    "        # get telemetry data\n",
    "        speed = telemetry['speed']\n",
    "        finished = telemetry['finished']\n",
    "        acceleration = telemetry['acceleration']\n",
    "        jerk = telemetry['jerk']\n",
    "        postion = telemetry['position']\n",
    "\n",
    "        reward = 0\n",
    "\n",
    "        # reward for speed > 100\n",
    "        # riding the wall gives 100 speed \n",
    "        # > 100 speed gives positive reward\n",
    "        s_speed = speed * 0.01  # scale speed 100 -> 1\n",
    "        reward += np.exp(s_speed) - 1  \n",
    "\n",
    "        # penalty for hitting wall\n",
    "        if round(jerk, 2) > 1:\n",
    "            print(f\"jerk: {round(jerk, 2)}\")\n",
    "            reward -= 1\n",
    "\n",
    "        # penalty for slow speed\n",
    "        if speed < 20:\n",
    "            reward -= 0.1\n",
    "\n",
    "        # reward for finishing \n",
    "        if finished:\n",
    "            reward += 100\n",
    "\n",
    "        # position based reward\n",
    "        # compare each step with the optimal positions\n",
    "        # fit a curved line to the optimal positions\n",
    "        # get the distance from the line to the current position\n",
    "        # reward based on the distance\n",
    "        \n",
    "\n",
    "        return reward\n",
    "        \n",
    "    \n",
    "    def make_env():\n",
    "        def _init():\n",
    "            env = TrackmaniaEnv()\n",
    "            return env\n",
    "        return _init\n",
    "\n",
    "    def close(self):\n",
    "        del self.window\n",
    "        self.client.close()  # Close the TMClient connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_positions = {}\n",
    "\n",
    "# map points\n",
    "class PlotPoints() :\n",
    "    def __init__(self):\n",
    "        super(PlotPoints, self).__init__()\n",
    "        self.client = client.TMClient()\n",
    "        self.gamepad = gp.GamepadHandler()\n",
    "        self.time_start = time.time()\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.gamepad.reset()\n",
    "        print(\"reset\")\n",
    "        time.sleep(1.5)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def step(self):\n",
    "        self.steps += 1\n",
    "        self.telemetry = self.client.retrieve_data()  # Retrieve the telemetry data\n",
    "        optimal_positions[self.steps] = self.telemetry['position']\n",
    "        # Process telemetry data to compute the reward and determine if the episode is done\n",
    "        \n",
    "        position = self.telemetry['position']\n",
    "        finished = self.telemetry['finished']\n",
    "        time_step = self.time_start - time.time()\n",
    "        \n",
    "        return position, time_step, finished\n",
    "\n",
    "\n",
    "\n",
    "    def close(self):\n",
    "        del self.window\n",
    "        self.client.close()  # Close the TMClient connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save positons to file\n",
    "def save_positions():\n",
    "    with open(\"models/positions.txt\", \"w\") as f:\n",
    "        for key in optimal_positions:\n",
    "            f.write(f\"{key}: {optimal_positions[key]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to localhost:9000 (Attempt 1/5)...\n",
      "Connected successfully!\n",
      "Gamepad initialized\n",
      "reset\n"
     ]
    }
   ],
   "source": [
    "env = PlotPoints()\n",
    "env.reset()\n",
    "\n",
    "while True:\n",
    "    env.step()\n",
    "    if env.telemetry['finished']:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWandbCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log only available metrics\n",
    "        wandb.log({\n",
    "            'reward': self.locals['rewards'],\n",
    "            'timesteps': self.num_timesteps\n",
    "        })\n",
    "        \n",
    "        # Log episode info if available\n",
    "        info = self.locals.get('infos', [{}])[0]\n",
    "        if info:\n",
    "            wandb.log({\n",
    "                'speed': info.get('speed', 0),\n",
    "                'checkpoint': info.get('checkpoint', 0),\n",
    "                'lap': info.get('lap', 0),\n",
    "                'episode_duration': info.get('episode_duration', 0)\n",
    "            })\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to localhost:9000 (Attempt 1/5)...\n",
      "Connected successfully!\n",
      "Gamepad initialized\n",
      "step: 100 / 1000000\n",
      "step: 200 / 1000000\n",
      "step: 300 / 1000000\n",
      "step: 400 / 1000000\n",
      "step: 500 / 1000000\n",
      "step: 600 / 1000000\n",
      "step: 700 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 763      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 0.343    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -20.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15262    |\n",
      "---------------------------------\n",
      "step: 800 / 1000000\n",
      "step: 900 / 1000000\n",
      "step: 1000 / 1000000\n",
      "jerk: 1.68\n",
      "step: 1100 / 1000000\n",
      "step: 1200 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 18       |\n",
      "|    time_elapsed    | 66       |\n",
      "|    total_timesteps | 1208     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.74    |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.00969  |\n",
      "|    ent_coef_loss   | -21.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15707    |\n",
      "---------------------------------\n",
      "step: 1300 / 1000000\n",
      "step: 1400 / 1000000\n",
      "jerk: 3.81\n",
      "step: 1500 / 1000000\n",
      "jerk: 1.07\n",
      "jerk: 1.45\n",
      "step: 1600 / 1000000\n",
      "step: 1700 / 1000000\n",
      "step: 1800 / 1000000\n",
      "jerk: 9.16\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 18       |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 1871     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.92    |\n",
      "|    critic_loss     | 0.173    |\n",
      "|    ent_coef        | 0.00808  |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16370    |\n",
      "---------------------------------\n",
      "step: 1900 / 1000000\n",
      "jerk: 2.15\n",
      "step: 2000 / 1000000\n",
      "step: 2100 / 1000000\n",
      "jerk: 4.44\n",
      "step: 2200 / 1000000\n",
      "step: 2300 / 1000000\n",
      "step: 2400 / 1000000\n",
      "step: 2500 / 1000000\n",
      "jerk: 6.72\n",
      "step: 2600 / 1000000\n",
      "step: 2700 / 1000000\n",
      "step: 2800 / 1000000\n",
      "jerk: 23.64\n",
      "step: 2900 / 1000000\n",
      "jerk: 4.36\n",
      "step: 3000 / 1000000\n",
      "jerk: 3.61\n",
      "step: 3100 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 18       |\n",
      "|    time_elapsed    | 167      |\n",
      "|    total_timesteps | 3166     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.31    |\n",
      "|    critic_loss     | 0.122    |\n",
      "|    ent_coef        | 0.00671  |\n",
      "|    ent_coef_loss   | -0.153   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17665    |\n",
      "---------------------------------\n",
      "step: 3200 / 1000000\n",
      "jerk: 1.82\n",
      "step: 3300 / 1000000\n",
      "step: 3400 / 1000000\n",
      "jerk: 3.7\n",
      "step: 3500 / 1000000\n",
      "step: 3600 / 1000000\n",
      "step: 3700 / 1000000\n",
      "step: 3800 / 1000000\n",
      "step: 3900 / 1000000\n",
      "jerk: 1.24\n",
      "step: 4000 / 1000000\n",
      "jerk: 1.18\n",
      "step: 4100 / 1000000\n",
      "jerk: 1.46\n",
      "step: 4200 / 1000000\n",
      "jerk: 3.1\n",
      "step: 4300 / 1000000\n",
      "step: 4400 / 1000000\n",
      "step: 4500 / 1000000\n",
      "jerk: 1.57\n",
      "step: 4600 / 1000000\n",
      "step: 4700 / 1000000\n",
      "step: 4800 / 1000000\n",
      "jerk: 8.15\n",
      "step: 4900 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 4947     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.98    |\n",
      "|    critic_loss     | 0.1      |\n",
      "|    ent_coef        | 0.00624  |\n",
      "|    ent_coef_loss   | -0.943   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19446    |\n",
      "---------------------------------\n",
      "step: 5000 / 1000000\n",
      "step: 5100 / 1000000\n",
      "step: 5200 / 1000000\n",
      "jerk: 3.12\n",
      "step: 5300 / 1000000\n",
      "jerk: 1.34\n",
      "step: 5400 / 1000000\n",
      "step: 5500 / 1000000\n",
      "step: 5600 / 1000000\n",
      "step: 5700 / 1000000\n",
      "jerk: 4.77\n",
      "jerk: 1.31\n",
      "step: 5800 / 1000000\n",
      "step: 5900 / 1000000\n",
      "jerk: 5.22\n",
      "step: 6000 / 1000000\n",
      "jerk: 1.61\n",
      "step: 6100 / 1000000\n",
      "jerk: 4.16\n",
      "step: 6200 / 1000000\n",
      "step: 6300 / 1000000\n",
      "jerk: 10.67\n",
      "jerk: 6.06\n",
      "step: 6400 / 1000000\n",
      "jerk: 2.21\n",
      "step: 6500 / 1000000\n",
      "step: 6600 / 1000000\n",
      "jerk: 13.89\n",
      "step: 6700 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 6712     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.47    |\n",
      "|    critic_loss     | 0.0798   |\n",
      "|    ent_coef        | 0.00568  |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21211    |\n",
      "---------------------------------\n",
      "step: 6800 / 1000000\n",
      "step: 6900 / 1000000\n",
      "jerk: 1.03\n",
      "step: 7000 / 1000000\n",
      "jerk: 5.93\n",
      "step: 7100 / 1000000\n",
      "step: 7200 / 1000000\n",
      "step: 7300 / 1000000\n",
      "jerk: 1.54\n",
      "step: 7400 / 1000000\n",
      "jerk: 2.75\n",
      "step: 7500 / 1000000\n",
      "step: 7600 / 1000000\n",
      "jerk: 8.31\n",
      "step: 7700 / 1000000\n",
      "jerk: 12.64\n",
      "jerk: 1.36\n",
      "step: 7800 / 1000000\n",
      "jerk: 5.54\n",
      "step: 7900 / 1000000\n",
      "step: 8000 / 1000000\n",
      "jerk: 4.57\n",
      "step: 8100 / 1000000\n",
      "step: 8200 / 1000000\n",
      "step: 8300 / 1000000\n",
      "step: 8400 / 1000000\n",
      "jerk: 1.76\n",
      "step: 8500 / 1000000\n",
      "step: 8600 / 1000000\n",
      "step: 8700 / 1000000\n",
      "step: 8800 / 1000000\n",
      "jerk: 6.57\n",
      "step: 8900 / 1000000\n",
      "jerk: 3.9\n",
      "jerk: 1.36\n",
      "step: 9000 / 1000000\n",
      "jerk: 1.91\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 457      |\n",
      "|    total_timesteps | 9090     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.24    |\n",
      "|    critic_loss     | 0.122    |\n",
      "|    ent_coef        | 0.00621  |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23589    |\n",
      "---------------------------------\n",
      "step: 9100 / 1000000\n",
      "jerk: 5.17\n",
      "step: 9200 / 1000000\n",
      "jerk: 11.29\n",
      "step: 9300 / 1000000\n",
      "jerk: 1.84\n",
      "step: 9400 / 1000000\n",
      "step: 9500 / 1000000\n",
      "jerk: 10.97\n",
      "step: 9600 / 1000000\n",
      "jerk: 1.43\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 491      |\n",
      "|    total_timesteps | 9682     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.89    |\n",
      "|    critic_loss     | 0.135    |\n",
      "|    ent_coef        | 0.00684  |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24181    |\n",
      "---------------------------------\n",
      "step: 9700 / 1000000\n",
      "jerk: 1.26\n",
      "step: 9800 / 1000000\n",
      "jerk: 2.39\n",
      "step: 9900 / 1000000\n",
      "jerk: 1.69\n",
      "step: 10000 / 1000000\n",
      "jerk: 1.52\n",
      "jerk: 1.05\n",
      "jerk: 1.08\n",
      "step: 10100 / 1000000\n",
      "jerk: 2.56\n",
      "step: 10200 / 1000000\n",
      "step: 10300 / 1000000\n",
      "jerk: 1.5\n",
      "step: 10400 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 10422    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.96    |\n",
      "|    critic_loss     | 0.195    |\n",
      "|    ent_coef        | 0.00663  |\n",
      "|    ent_coef_loss   | 0.556    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24921    |\n",
      "---------------------------------\n",
      "step: 10500 / 1000000\n",
      "jerk: 1.7\n",
      "jerk: 1.21\n",
      "step: 10600 / 1000000\n",
      "jerk: 1.46\n",
      "step: 10700 / 1000000\n",
      "jerk: 4.2\n",
      "step: 10800 / 1000000\n",
      "step: 10900 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 561      |\n",
      "|    total_timesteps | 10978    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.24    |\n",
      "|    critic_loss     | 0.119    |\n",
      "|    ent_coef        | 0.0067   |\n",
      "|    ent_coef_loss   | -0.0998  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25477    |\n",
      "---------------------------------\n",
      "step: 11000 / 1000000\n",
      "jerk: 3.02\n",
      "step: 11100 / 1000000\n",
      "jerk: 3.0\n",
      "jerk: 3.25\n",
      "step: 11200 / 1000000\n",
      "step: 11300 / 1000000\n",
      "jerk: 1.09\n",
      "step: 11400 / 1000000\n",
      "step: 11500 / 1000000\n",
      "jerk: 3.35\n",
      "step: 11600 / 1000000\n",
      "jerk: 20.0\n",
      "step: 11700 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 601      |\n",
      "|    total_timesteps | 11714    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.51    |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.00656  |\n",
      "|    ent_coef_loss   | -0.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26213    |\n",
      "---------------------------------\n",
      "step: 11800 / 1000000\n",
      "jerk: 1.06\n",
      "step: 11900 / 1000000\n",
      "jerk: 3.18\n",
      "step: 12000 / 1000000\n",
      "jerk: 11.64\n",
      "step: 12100 / 1000000\n",
      "jerk: 2.27\n",
      "step: 12200 / 1000000\n",
      "step: 12300 / 1000000\n",
      "step: 12400 / 1000000\n",
      "jerk: 11.55\n",
      "jerk: 1.83\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 642      |\n",
      "|    total_timesteps | 12494    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.07    |\n",
      "|    critic_loss     | 0.164    |\n",
      "|    ent_coef        | 0.00689  |\n",
      "|    ent_coef_loss   | -0.0334  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26993    |\n",
      "---------------------------------\n",
      "step: 12500 / 1000000\n",
      "step: 12600 / 1000000\n",
      "step: 12700 / 1000000\n",
      "step: 12800 / 1000000\n",
      "step: 12900 / 1000000\n",
      "step: 13000 / 1000000\n",
      "jerk: 3.02\n",
      "step: 13100 / 1000000\n",
      "jerk: 3.63\n",
      "step: 13200 / 1000000\n",
      "jerk: 3.89\n",
      "step: 13300 / 1000000\n",
      "jerk: 2.37\n",
      "step: 13400 / 1000000\n",
      "jerk: 3.25\n",
      "step: 13500 / 1000000\n",
      "jerk: 5.03\n",
      "step: 13600 / 1000000\n",
      "step: 13700 / 1000000\n",
      "jerk: 3.31\n",
      "step: 13800 / 1000000\n",
      "step: 13900 / 1000000\n",
      "jerk: 2.59\n",
      "step: 14000 / 1000000\n",
      "step: 14100 / 1000000\n",
      "jerk: 8.78\n",
      "jerk: 1.18\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 726      |\n",
      "|    total_timesteps | 14190    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.93    |\n",
      "|    critic_loss     | 0.132    |\n",
      "|    ent_coef        | 0.00742  |\n",
      "|    ent_coef_loss   | 0.409    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28689    |\n",
      "---------------------------------\n",
      "step: 14200 / 1000000\n",
      "jerk: 2.21\n",
      "step: 14300 / 1000000\n",
      "step: 14400 / 1000000\n",
      "jerk: 8.89\n",
      "jerk: 1.81\n",
      "step: 14500 / 1000000\n",
      "jerk: 5.16\n",
      "step: 14600 / 1000000\n",
      "step: 14700 / 1000000\n",
      "jerk: 2.08\n",
      "step: 14800 / 1000000\n",
      "jerk: 2.53\n",
      "step: 14900 / 1000000\n",
      "step: 15000 / 1000000\n",
      "jerk: 8.22\n",
      "step: 15100 / 1000000\n",
      "step: 15200 / 1000000\n",
      "step: 15300 / 1000000\n",
      "jerk: 1.01\n",
      "step: 15400 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 789      |\n",
      "|    total_timesteps | 15424    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.69    |\n",
      "|    critic_loss     | 0.167    |\n",
      "|    ent_coef        | 0.00889  |\n",
      "|    ent_coef_loss   | 0.156    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29923    |\n",
      "---------------------------------\n",
      "step: 15500 / 1000000\n",
      "step: 15600 / 1000000\n",
      "jerk: 1.67\n",
      "jerk: 14.68\n",
      "step: 15700 / 1000000\n",
      "step: 15800 / 1000000\n",
      "step: 15900 / 1000000\n",
      "jerk: 6.91\n",
      "step: 16000 / 1000000\n",
      "jerk: 1.88\n",
      "step: 16100 / 1000000\n",
      "step: 16200 / 1000000\n",
      "step: 16300 / 1000000\n",
      "jerk: 8.45\n",
      "jerk: 1.57\n",
      "step: 16400 / 1000000\n",
      "step: 16500 / 1000000\n",
      "jerk: 6.02\n",
      "step: 16600 / 1000000\n",
      "jerk: 20.24\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 852      |\n",
      "|    total_timesteps | 16677    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.84    |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.00916  |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31176    |\n",
      "---------------------------------\n",
      "step: 16700 / 1000000\n",
      "step: 16800 / 1000000\n",
      "step: 16900 / 1000000\n",
      "jerk: 17.35\n",
      "step: 17000 / 1000000\n",
      "step: 17100 / 1000000\n",
      "jerk: 2.93\n",
      "step: 17200 / 1000000\n",
      "step: 17300 / 1000000\n",
      "jerk: 6.23\n",
      "jerk: 2.47\n",
      "step: 17400 / 1000000\n",
      "step: 17500 / 1000000\n",
      "step: 17600 / 1000000\n",
      "jerk: 7.51\n",
      "jerk: 1.6\n",
      "step: 17700 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 906      |\n",
      "|    total_timesteps | 17718    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.9     |\n",
      "|    critic_loss     | 0.305    |\n",
      "|    ent_coef        | 0.00857  |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32217    |\n",
      "---------------------------------\n",
      "jerk: 1.79\n",
      "step: 17800 / 1000000\n",
      "step: 17900 / 1000000\n",
      "jerk: 6.12\n",
      "step: 18000 / 1000000\n",
      "jerk: 2.82\n",
      "step: 18100 / 1000000\n",
      "step: 18200 / 1000000\n",
      "jerk: 2.0\n",
      "step: 18300 / 1000000\n",
      "step: 18400 / 1000000\n",
      "step: 18500 / 1000000\n",
      "jerk: 11.84\n",
      "step: 18600 / 1000000\n",
      "step: 18700 / 1000000\n",
      "step: 18800 / 1000000\n",
      "jerk: 17.38\n",
      "step: 18900 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 967      |\n",
      "|    total_timesteps | 18908    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.45    |\n",
      "|    critic_loss     | 0.236    |\n",
      "|    ent_coef        | 0.00844  |\n",
      "|    ent_coef_loss   | 0.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33407    |\n",
      "---------------------------------\n",
      "step: 19000 / 1000000\n",
      "step: 19100 / 1000000\n",
      "jerk: 7.21\n",
      "step: 19200 / 1000000\n",
      "jerk: 4.77\n",
      "step: 19300 / 1000000\n",
      "step: 19400 / 1000000\n",
      "jerk: 18.6\n",
      "step: 19500 / 1000000\n",
      "jerk: 2.67\n",
      "step: 19600 / 1000000\n",
      "step: 19700 / 1000000\n",
      "jerk: 1.12\n",
      "step: 19800 / 1000000\n",
      "jerk: 16.56\n",
      "jerk: 1.01\n",
      "step: 19900 / 1000000\n",
      "jerk: 2.21\n",
      "step: 20000 / 1000000\n",
      "jerk: 5.64\n",
      "step: 20100 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1032     |\n",
      "|    total_timesteps | 20183    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.02    |\n",
      "|    critic_loss     | 0.166    |\n",
      "|    ent_coef        | 0.00836  |\n",
      "|    ent_coef_loss   | -0.0306  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34682    |\n",
      "---------------------------------\n",
      "step: 20200 / 1000000\n",
      "step: 20300 / 1000000\n",
      "jerk: 1.08\n",
      "step: 20400 / 1000000\n",
      "jerk: 11.9\n",
      "step: 20500 / 1000000\n",
      "jerk: 2.86\n",
      "step: 20600 / 1000000\n",
      "step: 20700 / 1000000\n",
      "jerk: 17.71\n",
      "step: 20800 / 1000000\n",
      "jerk: 1.18\n",
      "step: 20900 / 1000000\n",
      "step: 21000 / 1000000\n",
      "jerk: 6.5\n",
      "step: 21100 / 1000000\n",
      "jerk: 7.54\n",
      "step: 21200 / 1000000\n",
      "step: 21300 / 1000000\n",
      "jerk: 3.8\n",
      "jerk: 8.93\n",
      "step: 21400 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1094     |\n",
      "|    total_timesteps | 21413    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.6     |\n",
      "|    critic_loss     | 0.163    |\n",
      "|    ent_coef        | 0.00832  |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35912    |\n",
      "---------------------------------\n",
      "jerk: 2.46\n",
      "step: 21500 / 1000000\n",
      "step: 21600 / 1000000\n",
      "jerk: 8.1\n",
      "jerk: 1.89\n",
      "step: 21700 / 1000000\n",
      "step: 21800 / 1000000\n",
      "step: 21900 / 1000000\n",
      "jerk: 2.48\n",
      "jerk: 7.31\n",
      "jerk: 2.02\n",
      "step: 22000 / 1000000\n",
      "jerk: 1.73\n",
      "step: 22100 / 1000000\n",
      "step: 22200 / 1000000\n",
      "jerk: 7.79\n",
      "step: 22300 / 1000000\n",
      "step: 22400 / 1000000\n",
      "step: 22500 / 1000000\n",
      "step: 22600 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1155     |\n",
      "|    total_timesteps | 22610    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.75    |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.00859  |\n",
      "|    ent_coef_loss   | 0.0938   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37109    |\n",
      "---------------------------------\n",
      "jerk: 4.18\n",
      "step: 22700 / 1000000\n",
      "step: 22800 / 1000000\n",
      "step: 22900 / 1000000\n",
      "jerk: 2.68\n",
      "step: 23000 / 1000000\n",
      "step: 23100 / 1000000\n",
      "jerk: 10.76\n",
      "jerk: 1.8\n",
      "step: 23200 / 1000000\n",
      "jerk: 2.84\n",
      "jerk: 2.32\n",
      "step: 23300 / 1000000\n",
      "step: 23400 / 1000000\n",
      "jerk: 8.85\n",
      "step: 23500 / 1000000\n",
      "jerk: 2.72\n",
      "step: 23600 / 1000000\n",
      "step: 23700 / 1000000\n",
      "jerk: 14.58\n",
      "step: 23800 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1217     |\n",
      "|    total_timesteps | 23822    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.29    |\n",
      "|    critic_loss     | 0.223    |\n",
      "|    ent_coef        | 0.00849  |\n",
      "|    ent_coef_loss   | -0.939   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38321    |\n",
      "---------------------------------\n",
      "jerk: 1.6\n",
      "step: 23900 / 1000000\n",
      "step: 24000 / 1000000\n",
      "jerk: 5.59\n",
      "jerk: 6.56\n",
      "step: 24100 / 1000000\n",
      "jerk: 6.34\n",
      "step: 24200 / 1000000\n",
      "step: 24300 / 1000000\n",
      "step: 24400 / 1000000\n",
      "jerk: 10.5\n",
      "step: 24500 / 1000000\n",
      "step: 24600 / 1000000\n",
      "jerk: 9.44\n",
      "step: 24700 / 1000000\n",
      "jerk: 6.76\n",
      "jerk: 2.33\n",
      "step: 24800 / 1000000\n",
      "step: 24900 / 1000000\n",
      "jerk: 5.67\n",
      "step: 25000 / 1000000\n",
      "jerk: 10.87\n",
      "jerk: 2.25\n",
      "step: 25100 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1282     |\n",
      "|    total_timesteps | 25111    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.29    |\n",
      "|    critic_loss     | 0.233    |\n",
      "|    ent_coef        | 0.00918  |\n",
      "|    ent_coef_loss   | -0.279   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39610    |\n",
      "---------------------------------\n",
      "step: 25200 / 1000000\n",
      "jerk: 1.31\n",
      "step: 25300 / 1000000\n",
      "jerk: 17.9\n",
      "step: 25400 / 1000000\n",
      "jerk: 2.38\n",
      "step: 25500 / 1000000\n",
      "step: 25600 / 1000000\n",
      "jerk: 1.18\n",
      "step: 25700 / 1000000\n",
      "jerk: 2.13\n",
      "step: 25800 / 1000000\n",
      "step: 25900 / 1000000\n",
      "jerk: 6.18\n",
      "step: 26000 / 1000000\n",
      "jerk: 2.94\n",
      "jerk: 6.0\n",
      "step: 26100 / 1000000\n",
      "step: 26200 / 1000000\n",
      "jerk: 1.83\n",
      "step: 26300 / 1000000\n",
      "jerk: 2.49\n",
      "jerk: 3.11\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1343     |\n",
      "|    total_timesteps | 26377    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.45    |\n",
      "|    critic_loss     | 0.166    |\n",
      "|    ent_coef        | 0.00978  |\n",
      "|    ent_coef_loss   | -0.0577  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40876    |\n",
      "---------------------------------\n",
      "step: 26400 / 1000000\n",
      "jerk: 3.27\n",
      "step: 26500 / 1000000\n",
      "jerk: 10.52\n",
      "step: 26600 / 1000000\n",
      "jerk: 9.46\n",
      "step: 26700 / 1000000\n",
      "step: 26800 / 1000000\n",
      "jerk: 1.29\n",
      "jerk: 1.13\n",
      "step: 26900 / 1000000\n",
      "step: 27000 / 1000000\n",
      "jerk: 13.42\n",
      "step: 27100 / 1000000\n",
      "jerk: 1.61\n",
      "step: 27200 / 1000000\n",
      "step: 27300 / 1000000\n",
      "jerk: 5.1\n",
      "step: 27400 / 1000000\n",
      "jerk: 1.41\n",
      "step: 27500 / 1000000\n",
      "jerk: 12.4\n",
      "step: 27600 / 1000000\n",
      "jerk: 4.4\n",
      "step: 27700 / 1000000\n",
      "step: 27800 / 1000000\n",
      "jerk: 6.24\n",
      "jerk: 18.06\n",
      "jerk: 1.55\n",
      "step: 27900 / 1000000\n",
      "jerk: 1.08\n",
      "step: 28000 / 1000000\n",
      "step: 28100 / 1000000\n",
      "step: 28200 / 1000000\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 19       |\n",
      "|    time_elapsed    | 1432     |\n",
      "|    total_timesteps | 28259    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.08    |\n",
      "|    critic_loss     | 0.134    |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | -0.391   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42758    |\n",
      "---------------------------------\n",
      "step: 28300 / 1000000\n",
      "jerk: 1.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     15\u001b[0m     model \u001b[38;5;241m=\u001b[39m SAC(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCnnPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     16\u001b[0m                 env, \n\u001b[1;32m     17\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     18\u001b[0m                 buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500_000\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/trackmania_sac4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# cleanup\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m, in \u001b[0;36mTrackmaniaEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Send the action to the game\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#print(action)\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamepad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_action \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m     57\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mcapture()  \u001b[38;5;66;03m# Capture the current image\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/trackmania/trackmaniarl/gamepad.py:34\u001b[0m, in \u001b[0;36mGamepadHandler.send_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# gamepad state\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamepad\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 1_000_000\n",
    "load = True\n",
    "\n",
    "# creat env\n",
    "env = TrackmaniaEnv()\n",
    "env.reset()\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "# load model\n",
    "if load:\n",
    "    model = SAC.load(\"models/trackmania_sac3.zip\")\n",
    "    model.set_env(env)\n",
    "else:\n",
    "    # if not loading, new model\n",
    "    model = SAC('CnnPolicy', \n",
    "                env, \n",
    "                verbose=1,\n",
    "                buffer_size=500_000)\n",
    "\n",
    "# train\n",
    "model.learn(total_timesteps=steps, callback=CustomWandbCallback())\n",
    "model.save(\"models/trackmania_sac4\")\n",
    "\n",
    "# cleanup\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
