{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gettelemetry as client\n",
    "import gamepad as gp\n",
    "from gamepad import GamepadHandler\n",
    "import window as gwd\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import mss\n",
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import pywinctl as gw\n",
    "import vgamepad as vg\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.policies import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import CallbackList\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "\n",
    "import logging\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mauschra3\u001b[0m (\u001b[33mauschra3-massachusetts-institute-of-technology\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/auschra/ml/trackmania/trackmaniarl/wandb/run-20250104_154400-vvg3lq3u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/vvg3lq3u' target=\"_blank\">wobbly-microwave-268</a></strong> to <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac' target=\"_blank\">https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/vvg3lq3u' target=\"_blank\">https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/vvg3lq3u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/auschra3-massachusetts-institute-of-technology/trackmania_sac/runs/vvg3lq3u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x79b6f4f87ef0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()\n",
    "wandb.init(project=\"trackmania_sac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowCap():\n",
    "    '''find the trackmania window and \n",
    "    return gray normalised scaled pixel values'''\n",
    "    def __init__(self, window_name):\n",
    "        self.window_name = window_name\n",
    "        self.window = gw.getWindowsWithTitle(window_name)\n",
    "        if not self.window:\n",
    "            raise Exception(f\"Window with name '{window_name}' not found.\")\n",
    "        self.window = self.window[0]\n",
    "        self.top = self.window.top\n",
    "        self.left = self.window.left\n",
    "        self.width = self.window.width\n",
    "        self.height = self.window.height\n",
    "        self.monitor = {\"top\": self.top, \"left\": self.left, \"width\": self.width, \"height\": self.height}\n",
    "        self.sct = mss.mss()\n",
    "        self.resize = 128\n",
    "\n",
    "    def capture(self):\n",
    "        img = np.array(self.sct.grab(self.monitor))\n",
    "        img = cv2.resize(img, (self.resize, self.resize))       # resize\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)        # Convert to grayscale\n",
    "        img = img / 255.0           # normalize\n",
    "        return img\n",
    "\n",
    "    def __del__(self):\n",
    "        self.sct.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1947, 2)\n",
      "[[496.       624.004211]\n",
      " [496.       624.014954]\n",
      " [496.       624.032104]\n",
      " ...\n",
      " [385.598175 556.279663]\n",
      " [384.094574 556.857178]\n",
      " [382.578369 557.407227]]\n"
     ]
    }
   ],
   "source": [
    "# demo positions for position comparison\n",
    "with open('models/optimal_positions_2.txt', 'r') as f:\n",
    "    optimal_positions = f.readlines()\n",
    "\n",
    "# write x and z to list as tuple of floats\n",
    "# {'x': 495.9945068359375, 'y': 10.008896827697754, 'z': 631.3551025390625}\n",
    "optimal_positions_list = []\n",
    "\n",
    "# for each line in the file\n",
    "for pos in optimal_positions:\n",
    "    pos = pos.split(',')\n",
    "    # remove /n and {}\n",
    "    pos[0] = pos[0].replace('{', '')\n",
    "    pos[2] = pos[2].replace('}', '')\n",
    "    pos[0] = pos[0].replace('\\'', '')\n",
    "    pos[2] = pos[2].replace('\\'', '')\n",
    "\n",
    "    x = round(float(pos[0].split(':')[1]), 6)\n",
    "    z = round(float(pos[2].split(':')[1]), 6)\n",
    "    optimal_positions_list.append((x, z))\n",
    "\n",
    "# turn into array\n",
    "optimal_positions_array = np.array(optimal_positions_list)      # array vs list\n",
    "print(optimal_positions_array.shape)\n",
    "print(optimal_positions_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackmaniaEnv(gym.Env) :\n",
    "    def __init__(self, window_name=\"Trackmania\"):\n",
    "        super(TrackmaniaEnv, self).__init__()\n",
    "        self.window = WindowCap(window_name)        # capture window\n",
    "        self.client = client.TMClient()             # start client connection with openplanet server\n",
    "        self.gamepad = GamepadHandler()           # init gamepad\n",
    "        if not self.gamepad:\n",
    "            print(\"Failed to initialize gamepad\")\n",
    "\n",
    "        self.action_space = spaces.Box(\n",
    "        low=np.array([-1.0, 0.0, 0.0]),\n",
    "        high=np.array([1.0, 1.0, 1.0]),\n",
    "        dtype=np.float32\n",
    "        )\n",
    "    \n",
    "        # obs space\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,  \n",
    "            shape=(1, 128, 128),\n",
    "            dtype=np.uint8 \n",
    "        )\n",
    "\n",
    "        self.id = gwd.get_window_id(\"Trackmania\")           \n",
    "        self.focus = gwd.focus_window(self.id)                      # focus on window\n",
    "        self.reward_range = (-np.inf, np.inf)           \n",
    "        self.metadata = {'render.modes': ['human']}\n",
    "        self.terminated = False                                     # check if race ended\n",
    "        self.truncated = False                                      # dont know what the difference is\n",
    "        self.reward = 0                                             # reset reward\n",
    "        self.prev_action = np.array([0, 0, 0])                      # init action \n",
    "        self.prev_obs = np.zeros((1, 128, 128), dtype=np.uint8)     # init obs\n",
    "        \n",
    "        self.telemetry = self.client.retrieve_data()                # get the telemetry data from the server\n",
    "\n",
    "        self.datapoints = optimal_positions_list.copy()            # get human driven data\n",
    "        self.prev_position = None\n",
    "        self.previous_closest_index = -1  \n",
    "\n",
    "        self.speed_buffer = []                                      # keep 'x' frames speed buffer\n",
    "        self.steps = 0                                              # count training steps\n",
    "        self.episode_reward = 0                                     # get cumulative reward for the episode\n",
    "        self.episode_steps = 0\n",
    "        self.checkpoint = 0                                         \n",
    "        self.episode_start_time = None\n",
    "        self.step_start_time = None\n",
    "\n",
    "\n",
    "    def reset(self, seed = None):\n",
    "        self.gamepad.reset()                                                                                    \n",
    "        time.sleep(1.5)                                             # wait for lights to go green\n",
    "        self.focus = gwd.focus_window(self.id)                      # unsure if necessary\n",
    "\n",
    "        # reset counters for episode \n",
    "        self.episode_steps = 0    \n",
    "        self.episode_reward = 0                                  \n",
    "        self.reward = 0\n",
    "        self.episode_start_time = time.time()                      \n",
    "        self.step_start_time = time.time()                          \n",
    "        self.speed_buffer = []                                      \n",
    "        self.datapoints = optimal_positions_array.copy()                  # renew the checkpoint list\n",
    "        self.terminated = False\n",
    "        self.prev_action = np.array([0, 0, 0])\n",
    "        self.previous_closest_index = -1  \n",
    "        \n",
    "        obs = self.window.capture()\n",
    "        obs = np.expand_dims(obs, axis=0)                           # add channel dim for gym\n",
    "\n",
    "        self.episode_start_time = time.time()\n",
    "        self.step_start_time = time.time()\n",
    "\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        \n",
    "        return obs, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # update new step\n",
    "        self.steps += 1\n",
    "        self.episode_steps += 1\n",
    "\n",
    "        self.gamepad.send_action(action)                    # send action to gamepad and update prev action\n",
    "        self.prev_action = action\n",
    "\n",
    "\n",
    "        self.step_time = time.time() - self.step_start_time         # count step time\n",
    "        self.step_start_time = time.time()\n",
    "            \n",
    "        obs = self.window.capture()                         # capture new window\n",
    "\n",
    "        self.telemetry = self.client.retrieve_data()        # get telemetry data# \n",
    "        truncated = self._check_truncated(self.telemetry)   # check if car has crashed\n",
    "        if truncated:\n",
    "            print(\"truncated\")\n",
    "        terminated = self._check_finished(self.telemetry)   # check if car has finished, if so press A\n",
    "        if terminated:\n",
    "            self.gamepad.press_a()\n",
    "            self.terminated = True\n",
    "            print(\"terminated\")\n",
    "\n",
    "        reward = self.get_reward(self.telemetry)            # get reward\n",
    "        self.episode_reward += reward\n",
    "        self.prev_position = self.telemetry['position']\n",
    "\n",
    "        if self.steps % 10000 == 0:                         # log data\n",
    "            print(f\"step: {self.steps} / {steps}\")\n",
    "\n",
    "        info = {                \n",
    "            'speed': self.telemetry['speed'],\n",
    "            'position': self.telemetry['position'],\n",
    "            'checkpoint': self.telemetry['checkpoint'],\n",
    "            'lap': self.telemetry['lap'],\n",
    "            'episode reward': self.episode_reward,}         # log telemetry data\n",
    "\n",
    "        #print(obs.shape, reward, terminated, info)\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def _check_truncated(self, telemetry):\n",
    "        '''check if car has crashed into a wall or off the track'''\n",
    "        speed = telemetry['speed']\n",
    "        acceleration = telemetry['acceleration']\n",
    "        self.speed_buffer.append(speed)\n",
    "\n",
    "        if len(self.speed_buffer) > 50:\n",
    "            self.speed_buffer.pop(0)\n",
    "        speed_av = sum(self.speed_buffer) / len(self.speed_buffer)\n",
    "        if speed_av < 2 and acceleration <0.1 and time.time() - self.episode_start_time > 3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _check_finished(self, telemetry):  \n",
    "        '''check if car has finished the track'''\n",
    "        finished = telemetry['finished']\n",
    "        return finished\n",
    "\n",
    "    # get the closest demo point to current position \n",
    "    def _closest_point(self, car_position):\n",
    "        car_position = np.array([self.telemetry['position']['x'], self.telemetry['position']['z']]) # extract x and z\n",
    "        distances = np.linalg.norm(self.datapoints - car_position, axis=1)      # compare all points to current position, probably inefficient\n",
    "        closest_index = np.argmin(distances)        # get index of closest point.                    \n",
    "        return closest_index, self.datapoints[closest_index]        # (index, (x, z))\n",
    "    \n",
    "    # update the progress along the track\n",
    "    def _update_progress(self, car_position):\n",
    "        print(f\"update progress {self.steps}\")\n",
    "        car_position = np.array([self.telemetry['position']['x'], self.telemetry['position']['z']]) # extract x and z\n",
    "        '''main reward mechanism: calculate distance from last position to trajectory of '''\n",
    "        progress_reward = 0\n",
    "        # get closest point to current position\n",
    "\n",
    "\n",
    "        # get distance along trajectory from last checkpoint to next checkpoint\n",
    "        x, z = round(float(car_position['x']), 7), round(float(car_position['z']), 7)\n",
    "        x1, z1 = self.datapoints[0][0], self.datapoints[0][1]\n",
    "        x2, z2 = self.datapoints[1][0], self.datapoints[1][1]\n",
    "\n",
    "        dx = x2 - x1\n",
    "        dz = z2 - z1\n",
    "        vx = x - x1\n",
    "        vz = z - z1\n",
    "\n",
    "        # get distance^2 x1, z1 to x2, z2\n",
    "        dist_sq = dx**2 + dz**2\n",
    "        dot_prod = dx * vx + dz * vz\n",
    "\n",
    "        # projection\n",
    "        t = dot_prod / dist_sq\n",
    "        \n",
    "        # if -ve, position behind this vector, so wait until passed x1, z1\n",
    "        if t > 0 and dist_sq > 0.01 :\n",
    "            self.datapoints.pop(0)\n",
    "            progress_reward = min(t, 10)\n",
    "            #print(f\"dist {dist_sq}\")\n",
    "            #print(f\"dot {dot_prod}\")\n",
    "            #print(checkpoint_reward)\n",
    "        \n",
    "        return progress_reward\n",
    "\n",
    "    def get_reward(self, telemetry):\n",
    "        print(f\"reward, {self.steps}\")\n",
    "        # get telemetry data\n",
    "        speed = telemetry['speed']\n",
    "        finished = telemetry['finished']\n",
    "        jerk = telemetry['jerk']\n",
    "        steer = telemetry['steer']\n",
    "        position = telemetry['position']\n",
    "        reward = 0\n",
    "\n",
    "        speed_reward = speed / 3000 # max speed\n",
    "        progress_reward = 0 #self.update_progress(position)    # get progress reward\n",
    "        jerk_reward = -jerk * 0.01  # penalize jerk\n",
    "\n",
    "        print(f\"speed: {speed_reward}, progress_reward {progress_reward}, jerk_reward {jerk_reward}\")\n",
    "\n",
    "        reward = speed_reward + progress_reward + jerk_reward\n",
    "        print(f\"reward: {reward}\")\n",
    "        return reward\n",
    "        \n",
    "    \n",
    "    def make_env():\n",
    "        def _init():\n",
    "            env = TrackmaniaEnv()\n",
    "            return env\n",
    "        return _init\n",
    "\n",
    "    def close(self):\n",
    "        del self.window\n",
    "        self.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomWandbCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        # Log only available metrics\n",
    "        wandb.log({\n",
    "            'reward': self.locals['rewards'],\n",
    "            'timesteps': self.num_timesteps\n",
    "        })\n",
    "        \n",
    "        # Log episode info if available\n",
    "        info = self.locals.get('infos', [{}])[0]\n",
    "        if info:\n",
    "            wandb.log({\n",
    "                'speed': info.get('speed', 0),\n",
    "                'checkpoint': info.get('checkpoint', 0),\n",
    "                'lap': info.get('lap', 0),\n",
    "                'episode_duration': info.get('episode_duration', 0)\n",
    "            })\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to connect to localhost:9000 (Attempt 1)...\n",
      "Connected successfully!\n",
      "Gamepad initialized\n",
      "Using cuda device\n",
      "reward, 1\n",
      "speed: 0.0, progress_reward 0, jerk_reward -6.052394863218069e-05\n",
      "reward: -6.052394863218069e-05\n",
      "reward, 2\n",
      "speed: 6.626667082309723e-05, progress_reward 0, jerk_reward -0.0010184941440820694\n",
      "reward: -0.0009522274732589722\n",
      "reward, 3\n",
      "speed: 0.00017266666889190674, progress_reward 0, jerk_reward -0.0019961756467819216\n",
      "reward: -0.0018235089778900148\n",
      "reward, 4\n",
      "speed: 0.0002679666678110758, progress_reward 0, jerk_reward 7.382065057754517e-05\n",
      "reward: 0.000341787318388621\n",
      "reward, 5\n",
      "speed: 0.00027510001262029014, progress_reward 0, jerk_reward 0.001679118573665619\n",
      "reward: 0.0019542185862859093\n",
      "reward, 6\n",
      "speed: 0.00025950000683466594, progress_reward 0, jerk_reward 0.001467638611793518\n",
      "reward: 0.0017271386186281839\n",
      "reward, 7\n",
      "speed: 0.00035266669591267906, progress_reward 0, jerk_reward -0.0014387267827987672\n",
      "reward: -0.0010860600868860882\n",
      "reward, 8\n",
      "speed: 0.00034056663513183595, progress_reward 0, jerk_reward 0.00013634979724884034\n",
      "reward: 0.00047691643238067627\n",
      "reward, 9\n",
      "speed: 0.00043280001481374106, progress_reward 0, jerk_reward -7.438421249389649e-05\n",
      "reward: 0.00035841580231984457\n",
      "reward, 10\n",
      "speed: 0.0004376666943232218, progress_reward 0, jerk_reward -0.0002634811401367188\n",
      "reward: 0.00017418555418650304\n",
      "reward, 11\n",
      "speed: 0.0004304666916529338, progress_reward 0, jerk_reward 0.0016780614852905274\n",
      "reward: 0.0021085281769434613\n",
      "reward, 12\n",
      "speed: 0.00041586669286092123, progress_reward 0, jerk_reward -1.8054246902465822e-05\n",
      "reward: 0.00039781244595845543\n",
      "reward, 13\n",
      "speed: 0.00038673333326975505, progress_reward 0, jerk_reward 0.0006193304061889648\n",
      "reward: 0.00100606373945872\n",
      "reward, 14\n",
      "speed: 0.0004844667116800944, progress_reward 0, jerk_reward -0.0020408713817596435\n",
      "reward: -0.001556404670079549\n",
      "reward, 15\n",
      "speed: 0.0005754000345865886, progress_reward 0, jerk_reward -0.0028149962425231935\n",
      "reward: -0.002239596207936605\n",
      "reward, 16\n",
      "speed: 0.000663266658782959, progress_reward 0, jerk_reward 0.0016318392753601075\n",
      "reward: 0.0022951059341430663\n",
      "reward, 17\n",
      "speed: 0.000663700024286906, progress_reward 0, jerk_reward 0.0018614232540130615\n",
      "reward: 0.0025251232782999675\n",
      "reward, 18\n",
      "speed: 0.000627333402633667, progress_reward 0, jerk_reward 0.001785815954208374\n",
      "reward: 0.002413149356842041\n",
      "reward, 19\n",
      "speed: 0.0006155000527699789, progress_reward 0, jerk_reward -0.000911318063735962\n",
      "reward: -0.0002958180109659831\n",
      "reward, 20\n",
      "speed: 0.0006038333574930827, progress_reward 0, jerk_reward -2.7489662170410155e-06\n",
      "reward: 0.0006010843912760416\n",
      "reward, 21\n",
      "speed: 0.0005940000613530477, progress_reward 0, jerk_reward -1.9489526748657228e-05\n",
      "reward: 0.0005745105346043905\n",
      "reward, 22\n",
      "speed: 0.0006752667427062988, progress_reward 0, jerk_reward -0.001742687225341797\n",
      "reward: -0.0010674204826354983\n",
      "reward, 23\n",
      "speed: 0.0007403333981831869, progress_reward 0, jerk_reward -0.0016778945922851563\n",
      "reward: -0.0009375611941019695\n",
      "reward, 24\n",
      "speed: 0.0006534000237782796, progress_reward 0, jerk_reward 0.004154433012008667\n",
      "reward: 0.004807833035786947\n",
      "reward, 25\n",
      "speed: 0.0005670000314712525, progress_reward 0, jerk_reward 0.0032873857021331787\n",
      "reward: 0.003854385733604431\n",
      "reward, 26\n",
      "speed: 0.0005531667073567708, progress_reward 0, jerk_reward -0.003914115428924561\n",
      "reward: -0.00336094872156779\n",
      "reward, 27\n",
      "speed: 0.0005368666648864746, progress_reward 0, jerk_reward -8.902192115783691e-05\n",
      "reward: 0.00044784474372863773\n",
      "reward, 28\n",
      "speed: 0.0004730999867121379, progress_reward 0, jerk_reward 0.0017671358585357665\n",
      "reward: 0.0022402358452479044\n",
      "reward, 29\n",
      "speed: 0.0004581666787465413, progress_reward 0, jerk_reward -0.0016843652725219726\n",
      "reward: -0.0012261985937754313\n",
      "reward, 30\n",
      "speed: 0.0004350666602452596, progress_reward 0, jerk_reward 0.0002672100067138672\n",
      "reward: 0.0007022766669591268\n",
      "reward, 31\n",
      "speed: 0.00042100000381469724, progress_reward 0, jerk_reward -0.00020364761352539062\n",
      "reward: 0.00021735239028930663\n",
      "reward, 32\n",
      "speed: 0.0005276666879653931, progress_reward 0, jerk_reward -0.0036842215061187746\n",
      "reward: -0.0031565548181533816\n",
      "reward, 33\n",
      "speed: 0.0005037000179290771, progress_reward 0, jerk_reward 0.0038771891593933107\n",
      "reward: 0.0043808891773223875\n",
      "reward, 34\n",
      "speed: 0.0004810000658035278, progress_reward 0, jerk_reward 0.00031508564949035646\n",
      "reward: 0.0007960857152938842\n",
      "reward, 35\n",
      "speed: 0.0005091333786646525, progress_reward 0, jerk_reward -0.0021034085750579836\n",
      "reward: -0.001594275196393331\n",
      "reward, 36\n",
      "speed: 0.00048433335622151695, progress_reward 0, jerk_reward 0.0017383027076721192\n",
      "reward: 0.002222636063893636\n",
      "reward, 37\n",
      "speed: 0.0005424333810806275, progress_reward 0, jerk_reward -0.0017369484901428223\n",
      "reward: -0.0011945151090621948\n",
      "reward, 38\n",
      "speed: 0.0006286667188008626, progress_reward 0, jerk_reward -0.0017083239555358888\n",
      "reward: -0.001079657236735026\n",
      "reward, 39\n",
      "speed: 0.0007000000476837158, progress_reward 0, jerk_reward 0.0004482734203338623\n",
      "reward: 0.001148273468017578\n",
      "reward, 40\n",
      "speed: 0.0007460000514984131, progress_reward 0, jerk_reward 0.000990363359451294\n",
      "reward: 0.001736363410949707\n",
      "reward, 41\n",
      "speed: 0.0006984666983286539, progress_reward 0, jerk_reward 0.00271864652633667\n",
      "reward: 0.003417113224665324\n",
      "reward, 42\n",
      "speed: 0.0007395334243774414, progress_reward 0, jerk_reward -0.0021813130378723145\n",
      "reward: -0.0014417796134948732\n",
      "reward, 43\n",
      "speed: 0.0007777667045593262, progress_reward 0, jerk_reward -0.0006075429916381837\n",
      "reward: 0.00017022371292114253\n",
      "reward, 44\n",
      "speed: 0.0007633333206176758, progress_reward 0, jerk_reward 0.0018222403526306152\n",
      "reward: 0.002585573673248291\n",
      "reward, 45\n",
      "speed: 0.0007484000523885091, progress_reward 0, jerk_reward 9.435892105102539e-05\n",
      "reward: 0.0008427589734395345\n",
      "reward, 46\n",
      "speed: 0.000755566676457723, progress_reward 0, jerk_reward -0.00013283967971801757\n",
      "reward: 0.0006227269967397054\n",
      "reward, 47\n",
      "speed: 0.0007634333769480387, progress_reward 0, jerk_reward -0.0012467050552368165\n",
      "reward: -0.00048327167828877775\n",
      "reward, 48\n",
      "speed: 0.0006742000579833985, progress_reward 0, jerk_reward 0.0036627626419067382\n",
      "reward: 0.004336962699890137\n",
      "reward, 49\n",
      "speed: 0.0006412667036056519, progress_reward 0, jerk_reward -0.0017636430263519287\n",
      "reward: -0.0011223763227462768\n",
      "reward, 50\n",
      "speed: 0.0006776666641235352, progress_reward 0, jerk_reward -0.0016889309883117677\n",
      "reward: -0.0010112643241882325\n",
      "reward, 51\n",
      "speed: 0.0007416000366210938, progress_reward 0, jerk_reward -0.0015681350231170656\n",
      "reward: -0.0008265349864959718\n",
      "reward, 52\n",
      "speed: 0.0007010667324066162, progress_reward 0, jerk_reward 0.003581092357635498\n",
      "reward: 0.004282159090042114\n",
      "reward, 53\n",
      "speed: 0.0006900333563486735, progress_reward 0, jerk_reward -0.0009568929672241211\n",
      "reward: -0.0002668596108754476\n",
      "reward, 54\n",
      "speed: 0.0006749334335327149, progress_reward 0, jerk_reward 5.010128021240235e-05\n",
      "reward: 0.0007250347137451172\n",
      "reward, 55\n",
      "speed: 0.0006518000364303589, progress_reward 0, jerk_reward 0.00015241622924804687\n",
      "reward: 0.0008042162656784057\n",
      "reward, 56\n",
      "speed: 0.0006302000681559245, progress_reward 0, jerk_reward 8.529067039489746e-05\n",
      "reward: 0.000715490738550822\n",
      "reward, 57\n",
      "speed: 0.0006152000427246093, progress_reward 0, jerk_reward -7.854223251342774e-05\n",
      "reward: 0.0005366578102111816\n",
      "reward, 58\n",
      "speed: 0.0006053333679835001, progress_reward 0, jerk_reward -0.0002030181884765625\n",
      "reward: 0.0004023151795069376\n",
      "reward, 59\n",
      "speed: 0.0006618333657582601, progress_reward 0, jerk_reward -0.0013961219787597657\n",
      "reward: -0.0007342886130015056\n",
      "reward, 60\n",
      "speed: 0.0006733333269755045, progress_reward 0, jerk_reward 2.019047737121582e-05\n",
      "reward: 0.0006935238043467203\n",
      "reward, 61\n",
      "speed: 0.0006638333797454834, progress_reward 0, jerk_reward 0.0013184821605682373\n",
      "reward: 0.0019823155403137205\n",
      "reward, 62\n",
      "speed: 0.0006541666984558105, progress_reward 0, jerk_reward -3.560781478881836e-06\n",
      "reward: 0.0006506059169769287\n",
      "reward, 63\n",
      "speed: 0.0006224000453948974, progress_reward 0, jerk_reward 0.0004405498504638672\n",
      "reward: 0.0010629498958587647\n",
      "reward, 64\n",
      "speed: 0.0007003334363301595, progress_reward 0, jerk_reward -0.0017930996417999268\n",
      "reward: -0.0010927662054697673\n",
      "reward, 65\n",
      "speed: 0.0007958667278289795, progress_reward 0, jerk_reward -0.0019471395015716554\n",
      "reward: -0.001151272773742676\n",
      "reward, 66\n",
      "speed: 0.0009076000054677328, progress_reward 0, jerk_reward -7.445573806762696e-05\n",
      "reward: 0.0008331442674001058\n",
      "reward, 67\n",
      "speed: 0.0008820667266845703, progress_reward 0, jerk_reward 0.0022137451171875\n",
      "reward: 0.0030958118438720707\n",
      "reward, 68\n",
      "speed: 0.0008432000478108723, progress_reward 0, jerk_reward 0.002363131046295166\n",
      "reward: 0.0032063310941060387\n",
      "reward, 69\n",
      "speed: 0.0008019333680470785, progress_reward 0, jerk_reward -0.0005066442489624023\n",
      "reward: 0.00029528911908467616\n",
      "reward, 70\n",
      "speed: 0.0008512667020161947, progress_reward 0, jerk_reward -0.0013895702362060546\n",
      "reward: -0.00053830353418986\n",
      "reward, 71\n",
      "speed: 0.000830933411916097, progress_reward 0, jerk_reward 3.446340560913086e-05\n",
      "reward: 0.0008653968175252279\n",
      "reward, 72\n",
      "speed: 0.0008149333794911703, progress_reward 0, jerk_reward 0.0010287117958068849\n",
      "reward: 0.0018436451752980552\n",
      "reward, 73\n",
      "speed: 0.0008031334082285564, progress_reward 0, jerk_reward -0.00030098438262939455\n",
      "reward: 0.0005021490255991618\n",
      "reward, 74\n",
      "speed: 0.0007594000498453776, progress_reward 0, jerk_reward 0.0005253672599792481\n",
      "reward: 0.0012847673098246256\n",
      "reward, 75\n",
      "speed: 0.0008246667385101318, progress_reward 0, jerk_reward -0.0022983455657958987\n",
      "reward: -0.0014736788272857668\n",
      "reward, 76\n",
      "speed: 0.0008984333674112955, progress_reward 0, jerk_reward -0.0007004976272583008\n",
      "reward: 0.00019793574015299477\n",
      "reward, 77\n",
      "speed: 0.0009863334496816, progress_reward 0, jerk_reward -0.00046729326248168946\n",
      "reward: 0.0005190401871999104\n",
      "reward, 78\n",
      "speed: 0.0010796666940053304, progress_reward 0, jerk_reward -0.00037690877914428714\n",
      "reward: 0.0007027579148610433\n",
      "reward, 79\n",
      "speed: 0.0011763667265574137, progress_reward 0, jerk_reward 0.00018593072891235352\n",
      "reward: 0.0013622974554697673\n",
      "reward, 80\n",
      "speed: 0.001266200065612793, progress_reward 0, jerk_reward 0.00018401145935058595\n",
      "reward: 0.001450211524963379\n",
      "reward, 81\n",
      "speed: 0.0012535335222880045, progress_reward 0, jerk_reward 0.0023397135734558104\n",
      "reward: 0.0035932470957438147\n",
      "reward, 82\n",
      "speed: 0.0011858333746592204, progress_reward 0, jerk_reward 0.0021611857414245606\n",
      "reward: 0.003347019116083781\n",
      "reward, 83\n",
      "speed: 0.0011610668500264486, progress_reward 0, jerk_reward -0.0007419157028198243\n",
      "reward: 0.00041915114720662437\n",
      "reward, 84\n",
      "speed: 0.0011375001271565755, progress_reward 0, jerk_reward -0.0008181595802307129\n",
      "reward: 0.00031934054692586255\n",
      "reward, 85\n",
      "speed: 0.0011134000619252523, progress_reward 0, jerk_reward 0.0007042217254638672\n",
      "reward: 0.0018176217873891196\n",
      "reward, 86\n",
      "speed: 0.0011018667221069337, progress_reward 0, jerk_reward -0.0007124185562133789\n",
      "reward: 0.0003894481658935548\n",
      "reward, 87\n",
      "speed: 0.0011482667128245036, progress_reward 0, jerk_reward -0.0017477703094482423\n",
      "reward: -0.0005995035966237386\n",
      "reward, 88\n",
      "speed: 0.0011363334655761718, progress_reward 0, jerk_reward 0.0017545223236083984\n",
      "reward: 0.0028908557891845705\n",
      "reward, 89\n",
      "speed: 0.0010429333845774333, progress_reward 0, jerk_reward 0.0021046328544616698\n",
      "reward: 0.003147566239039103\n",
      "reward, 90\n",
      "speed: 0.0010117000738779703, progress_reward 0, jerk_reward -0.0011863851547241212\n",
      "reward: -0.0001746850808461509\n",
      "reward, 91\n",
      "speed: 0.0009670000076293945, progress_reward 0, jerk_reward -0.00029614210128784183\n",
      "reward: 0.0006708579063415527\n",
      "reward, 92\n",
      "speed: 0.0009502667586008708, progress_reward 0, jerk_reward -0.00017465114593505859\n",
      "reward: 0.0007756156126658122\n",
      "reward, 93\n",
      "speed: 0.0009294667243957519, progress_reward 0, jerk_reward -0.00031085968017578123\n",
      "reward: 0.0006186070442199708\n",
      "reward, 94\n",
      "speed: 0.0009273333549499512, progress_reward 0, jerk_reward -0.00012212038040161134\n",
      "reward: 0.0008052129745483398\n",
      "reward, 95\n",
      "speed: 0.0008688000837961833, progress_reward 0, jerk_reward 0.0011904335021972656\n",
      "reward: 0.002059233585993449\n",
      "reward, 96\n",
      "speed: 0.0008355666796366374, progress_reward 0, jerk_reward -0.00047600507736206054\n",
      "reward: 0.00035956160227457683\n",
      "reward, 97\n",
      "speed: 0.0008058667182922364, progress_reward 0, jerk_reward -0.00016720056533813476\n",
      "reward: 0.0006386661529541016\n",
      "reward, 98\n",
      "speed: 0.0008358667691548665, progress_reward 0, jerk_reward -0.0014480876922607422\n",
      "reward: -0.0006122209231058757\n",
      "reward, 99\n",
      "speed: 0.0008681333859761556, progress_reward 0, jerk_reward -0.0008084750175476075\n",
      "reward: 5.965836842854817e-05\n",
      "reward, 100\n",
      "speed: 0.0008544000784556071, progress_reward 0, jerk_reward 0.001718001365661621\n",
      "reward: 0.002572401444117228\n",
      "reward, 101\n",
      "speed: 0.000817400058110555, progress_reward 0, jerk_reward 0.0005364775657653809\n",
      "reward: 0.001353877623875936\n",
      "reward, 102\n",
      "speed: 0.00090256667137146, progress_reward 0, jerk_reward -0.0020154476165771484\n",
      "reward: -0.0011128809452056884\n",
      "reward, 103\n",
      "speed: 0.0009078667163848877, progress_reward 0, jerk_reward -0.00032140254974365233\n",
      "reward: 0.0005864641666412354\n",
      "reward, 104\n",
      "speed: 0.0008953333695729573, progress_reward 0, jerk_reward 0.0019555449485778808\n",
      "reward: 0.002850878318150838\n",
      "reward, 105\n",
      "speed: 0.0008834667205810546, progress_reward 0, jerk_reward -0.0001909184455871582\n",
      "reward: 0.0006925482749938964\n",
      "reward, 106\n",
      "speed: 0.0009213333924611409, progress_reward 0, jerk_reward -0.001234896183013916\n",
      "reward: -0.000313562790552775\n",
      "reward, 107\n",
      "speed: 0.000982666810353597, progress_reward 0, jerk_reward -0.0011879277229309081\n",
      "reward: -0.0002052609125773111\n",
      "reward, 108\n",
      "speed: 0.0009722667535146078, progress_reward 0, jerk_reward 0.002398843765258789\n",
      "reward: 0.0033711105187733967\n",
      "reward, 109\n",
      "speed: 0.000961666742960612, progress_reward 0, jerk_reward -3.957748413085938e-06\n",
      "reward: 0.0009577089945475261\n",
      "reward, 110\n",
      "speed: 0.0010108668009440104, progress_reward 0, jerk_reward -0.0014944005012512208\n",
      "reward: -0.0004835337003072104\n",
      "reward, 111\n",
      "speed: 0.0010861000219980876, progress_reward 0, jerk_reward -0.0013484382629394532\n",
      "reward: -0.0002623382409413656\n",
      "reward, 112\n",
      "speed: 0.0010515333811442057, progress_reward 0, jerk_reward 0.003533289432525635\n",
      "reward: 0.00458482281366984\n",
      "reward, 113\n",
      "speed: 0.0010194001197814941, progress_reward 0, jerk_reward -0.0002495765686035156\n",
      "reward: 0.0007698235511779785\n",
      "reward, 114\n",
      "speed: 0.0009898333549499512, progress_reward 0, jerk_reward 8.946657180786133e-05\n",
      "reward: 0.0010792999267578126\n",
      "reward, 115\n",
      "speed: 0.0010752333799997965, progress_reward 0, jerk_reward -0.002045295238494873\n",
      "reward: -0.0009700618584950766\n",
      "reward, 116\n",
      "speed: 0.001139600118001302, progress_reward 0, jerk_reward -0.002002208232879639\n",
      "reward: -0.0008626081148783368\n",
      "reward, 117\n",
      "speed: 0.0011193334261576334, progress_reward 0, jerk_reward 0.0037167596817016604\n",
      "reward: 0.004836093107859294\n",
      "reward, 118\n",
      "speed: 0.0010893333752950032, progress_reward 0, jerk_reward 0.0005907654762268066\n",
      "reward: 0.0016800988515218098\n",
      "reward, 119\n",
      "speed: 0.0010726667245229086, progress_reward 0, jerk_reward -0.0005378222465515137\n",
      "reward: 0.0005348444779713949\n",
      "reward, 120\n",
      "speed: 0.0009890000820159913, progress_reward 0, jerk_reward 0.001964988708496094\n",
      "reward: 0.002953988790512085\n",
      "reward, 121\n",
      "speed: 0.000956000010172526, progress_reward 0, jerk_reward -0.0015461969375610352\n",
      "reward: -0.0005901969273885092\n",
      "reward, 122\n",
      "speed: 0.000940666675567627, progress_reward 0, jerk_reward -0.0005244135856628418\n",
      "reward: 0.0004162530899047851\n",
      "reward, 123\n",
      "speed: 0.000913433313369751, progress_reward 0, jerk_reward 0.0002548098564147949\n",
      "reward: 0.001168243169784546\n",
      "reward, 124\n",
      "speed: 0.0009449334144592286, progress_reward 0, jerk_reward -0.0013600564002990722\n",
      "reward: -0.00041512298583984365\n",
      "reward, 125\n",
      "speed: 0.001032533327738444, progress_reward 0, jerk_reward -0.001878354549407959\n",
      "reward: -0.0008458212216695151\n",
      "reward, 126\n",
      "speed: 0.001133900006612142, progress_reward 0, jerk_reward -0.00039295196533203127\n",
      "reward: 0.0007409480412801108\n",
      "reward, 127\n",
      "speed: 0.0012196000417073567, progress_reward 0, jerk_reward 0.0001932668685913086\n",
      "reward: 0.0014128669102986653\n",
      "reward, 128\n",
      "speed: 0.0012909000714619954, progress_reward 0, jerk_reward 0.0006024456024169922\n",
      "reward: 0.0018933456738789876\n",
      "reward, 129\n",
      "speed: 0.0013025667667388916, progress_reward 0, jerk_reward 0.001400589942932129\n",
      "reward: 0.0027031567096710207\n",
      "reward, 130\n",
      "speed: 0.001288566748301188, progress_reward 0, jerk_reward 0.001178750991821289\n",
      "reward: 0.0024673177401224773\n",
      "reward, 131\n",
      "speed: 0.0013235334555308023, progress_reward 0, jerk_reward -0.0010825181007385254\n",
      "reward: 0.00024101535479227696\n",
      "reward, 132\n",
      "speed: 0.001318000078201294, progress_reward 0, jerk_reward -0.00012418031692504883\n",
      "reward: 0.001193819761276245\n",
      "reward, 133\n",
      "speed: 0.0012863334019978842, progress_reward 0, jerk_reward 0.0020876097679138183\n",
      "reward: 0.0033739431699117027\n",
      "reward, 134\n",
      "speed: 0.0012708667119344075, progress_reward 0, jerk_reward -0.0008765053749084472\n",
      "reward: 0.0003943613370259603\n",
      "reward, 135\n",
      "speed: 0.001253200054168701, progress_reward 0, jerk_reward 0.00011618375778198242\n",
      "reward: 0.0013693838119506836\n",
      "reward, 136\n",
      "speed: 0.0011934000651041667, progress_reward 0, jerk_reward 0.0005548644065856934\n",
      "reward: 0.0017482644716898602\n",
      "reward, 137\n",
      "speed: 0.0012799334526062012, progress_reward 0, jerk_reward -0.0017425870895385743\n",
      "reward: -0.0004626536369323731\n",
      "reward, 138\n",
      "speed: 0.0013318666617075602, progress_reward 0, jerk_reward -0.002522134780883789\n",
      "reward: -0.001190268119176229\n",
      "reward, 139\n",
      "speed: 0.0014423667589823406, progress_reward 0, jerk_reward 0.0003212285041809082\n",
      "reward: 0.0017635952631632488\n",
      "reward, 140\n",
      "speed: 0.001541733423868815, progress_reward 0, jerk_reward -0.0003827476501464844\n",
      "reward: 0.0011589857737223305\n",
      "reward, 141\n",
      "speed: 0.0014419666926066082, progress_reward 0, jerk_reward 0.005995988845825195\n",
      "reward: 0.007437955538431804\n",
      "reward, 142\n",
      "speed: 0.0013821334838867187, progress_reward 0, jerk_reward -0.0010793542861938477\n",
      "reward: 0.00030277919769287095\n",
      "reward, 143\n",
      "speed: 0.0013243335088094076, progress_reward 0, jerk_reward -2.5758743286132814e-05\n",
      "reward: 0.0012985747655232748\n",
      "reward, 144\n",
      "speed: 0.0012689334551493326, progress_reward 0, jerk_reward 0.00010826349258422852\n",
      "reward: 0.0013771969477335611\n",
      "reward, 145\n",
      "speed: 0.0012532666524251303, progress_reward 0, jerk_reward -0.0012615513801574707\n",
      "reward: -8.28472773234043e-06\n",
      "reward, 146\n",
      "speed: 0.0012481667200724283, progress_reward 0, jerk_reward -0.0002754783630371094\n",
      "reward: 0.0009726883570353189\n",
      "reward, 147\n",
      "speed: 0.001238133430480957, progress_reward 0, jerk_reward -0.00010103464126586914\n",
      "reward: 0.0011370987892150878\n",
      "reward, 148\n",
      "speed: 0.0011397333145141601, progress_reward 0, jerk_reward 0.002460615634918213\n",
      "reward: 0.0036003489494323727\n",
      "reward, 149\n",
      "speed: 0.0011175333658854166, progress_reward 0, jerk_reward -0.001633443832397461\n",
      "reward: -0.0005159104665120443\n",
      "reward, 150\n",
      "speed: 0.0010191334088643393, progress_reward 0, jerk_reward 0.0015420198440551757\n",
      "reward: 0.0025611532529195152\n",
      "reward, 151\n",
      "speed: 0.0009757334391276042, progress_reward 0, jerk_reward -0.0010846710205078126\n",
      "reward: -0.00010893758138020844\n",
      "reward, 152\n",
      "speed: 0.0009350667794545491, progress_reward 0, jerk_reward -0.00035567522048950194\n",
      "reward: 0.0005793915589650472\n",
      "reward, 153\n",
      "speed: 0.0008885334332784017, progress_reward 0, jerk_reward 0.00010138988494873047\n",
      "reward: 0.0009899233182271321\n",
      "reward, 154\n",
      "speed: 0.0008746333916982015, progress_reward 0, jerk_reward -0.0002142190933227539\n",
      "reward: 0.0006604142983754476\n",
      "reward, 155\n",
      "speed: 0.0008586000601450602, progress_reward 0, jerk_reward -0.0005909228324890137\n",
      "reward: 0.0002676772276560465\n",
      "reward, 156\n",
      "speed: 0.0008456667264302571, progress_reward 0, jerk_reward 0.00022908449172973633\n",
      "reward: 0.0010747512181599934\n",
      "reward, 157\n",
      "speed: 0.0009048666954040527, progress_reward 0, jerk_reward -0.002054562568664551\n",
      "reward: -0.0011496958732604983\n",
      "reward, 158\n",
      "speed: 0.0009397334257761638, progress_reward 0, jerk_reward -1.6956329345703126e-05\n",
      "reward: 0.0009227770964304607\n",
      "reward, 159\n",
      "speed: 0.0009229000409444173, progress_reward 0, jerk_reward 0.0019083023071289062\n",
      "reward: 0.0028312023480733233\n",
      "reward, 160\n",
      "speed: 0.0008617334365844726, progress_reward 0, jerk_reward 0.0006248784065246582\n",
      "reward: 0.0014866118431091308\n",
      "reward, 161\n",
      "speed: 0.0008098333676656087, progress_reward 0, jerk_reward 0.0012437701225280762\n",
      "reward: 0.002053603490193685\n",
      "reward, 162\n",
      "speed: 0.0007516667048136393, progress_reward 0, jerk_reward -0.0014472508430480957\n",
      "reward: -0.0006955841382344564\n",
      "reward, 163\n",
      "speed: 0.0008061333497365316, progress_reward 0, jerk_reward -0.00041239023208618165\n",
      "reward: 0.00039374311765034997\n",
      "reward, 164\n",
      "speed: 0.0009212334156036377, progress_reward 0, jerk_reward -0.003678531646728516\n",
      "reward: -0.002757298231124878\n",
      "reward, 165\n",
      "speed: 0.0010248666604359944, progress_reward 0, jerk_reward -0.001233694553375244\n",
      "reward: -0.00020882789293924964\n",
      "reward, 166\n",
      "speed: 0.001054466724395752, progress_reward 0, jerk_reward 0.003396084308624268\n",
      "reward: 0.0044505510330200195\n",
      "reward, 167\n",
      "speed: 0.0009672333399454753, progress_reward 0, jerk_reward 0.0035904335975646975\n",
      "reward: 0.004557666937510173\n",
      "reward, 168\n",
      "speed: 0.0009448333581288656, progress_reward 0, jerk_reward -0.0017039847373962402\n",
      "reward: -0.0007591513792673746\n",
      "reward, 169\n",
      "speed: 0.0009231000741322835, progress_reward 0, jerk_reward -0.00048806905746459964\n",
      "reward: 0.00043503101666768384\n",
      "reward, 170\n",
      "speed: 0.0008990667661031087, progress_reward 0, jerk_reward 0.00034238576889038084\n",
      "reward: 0.0012414525349934895\n",
      "reward, 171\n",
      "speed: 0.0008840000629425049, progress_reward 0, jerk_reward 0.00011575698852539062\n",
      "reward: 0.0009997570514678955\n",
      "reward, 172\n",
      "speed: 0.0009290000597635905, progress_reward 0, jerk_reward -0.0021588444709777833\n",
      "reward: -0.0012298444112141928\n",
      "reward, 173\n",
      "speed: 0.0009647667407989502, progress_reward 0, jerk_reward 0.00017708539962768555\n",
      "reward: 0.0011418521404266357\n",
      "reward, 174\n",
      "speed: 0.0009399000008900961, progress_reward 0, jerk_reward 0.0019259977340698243\n",
      "reward: 0.0028658977349599202\n",
      "reward, 175\n",
      "speed: 0.0009209667841593424, progress_reward 0, jerk_reward -0.00021117210388183594\n",
      "reward: 0.0007097946802775066\n",
      "reward, 176\n",
      "speed: 0.0009060000578562418, progress_reward 0, jerk_reward -3.9775371551513675e-05\n",
      "reward: 0.0008662246863047282\n",
      "reward, 177\n",
      "speed: 0.000858666737874349, progress_reward 0, jerk_reward -8.886337280273438e-05\n",
      "reward: 0.0007698033650716145\n",
      "reward, 178\n",
      "speed: 0.0008151667118072509, progress_reward 0, jerk_reward 0.0019479775428771972\n",
      "reward: 0.0027631442546844484\n",
      "reward, 179\n",
      "speed: 0.0008111333847045898, progress_reward 0, jerk_reward -0.0022942948341369627\n",
      "reward: -0.001483161449432373\n",
      "reward, 180\n",
      "speed: 0.0007997000217437744, progress_reward 0, jerk_reward 0.0002195429801940918\n",
      "reward: 0.0010192430019378663\n",
      "reward, 181\n",
      "speed: 0.000786133368810018, progress_reward 0, jerk_reward 7.771015167236328e-05\n",
      "reward: 0.0008638435204823813\n",
      "reward, 182\n",
      "speed: 0.0008760666847229003, progress_reward 0, jerk_reward -0.0016889929771423341\n",
      "reward: -0.0008129262924194338\n",
      "reward, 183\n",
      "speed: 0.0008632000287373861, progress_reward 0, jerk_reward 0.00043866634368896487\n",
      "reward: 0.001301866372426351\n",
      "reward, 184\n",
      "speed: 0.0008398667176564535, progress_reward 0, jerk_reward 0.0016217684745788575\n",
      "reward: 0.002461635192235311\n",
      "reward, 185\n",
      "speed: 0.0008207333882649739, progress_reward 0, jerk_reward -0.00027110815048217776\n",
      "reward: 0.0005496252377827961\n",
      "reward, 186\n",
      "speed: 0.0008221000035603841, progress_reward 0, jerk_reward 8.021831512451171e-05\n",
      "reward: 0.0009023183186848958\n",
      "reward, 187\n",
      "speed: 0.0009026001294453939, progress_reward 0, jerk_reward -0.003515293598175049\n",
      "reward: -0.002612693468729655\n",
      "reward, 188\n",
      "speed: 0.0008920000394185384, progress_reward 0, jerk_reward 0.0033069562911987307\n",
      "reward: 0.004198956330617269\n",
      "reward, 189\n",
      "speed: 0.0008804666996002198, progress_reward 0, jerk_reward -7.62939453125e-06\n",
      "reward: 0.0008728373050689698\n",
      "reward, 190\n",
      "speed: 0.0008703333536783855, progress_reward 0, jerk_reward -2.818107604980469e-06\n",
      "reward: 0.0008675152460734049\n",
      "reward, 191\n",
      "speed: 0.0008503333727518718, progress_reward 0, jerk_reward -5.333423614501953e-06\n",
      "reward: 0.0008449999491373699\n",
      "reward, 192\n",
      "speed: 0.0008025000890096029, progress_reward 0, jerk_reward 0.0011444950103759765\n",
      "reward: 0.0019469950993855794\n",
      "reward, 193\n",
      "speed: 0.0007845000425974528, progress_reward 0, jerk_reward -0.0006500792503356934\n",
      "reward: 0.00013442079226175946\n",
      "reward, 194\n",
      "speed: 0.00075, progress_reward 0, jerk_reward -2.6662349700927734e-05\n",
      "reward: 0.0007233376502990723\n",
      "reward, 195\n",
      "speed: 0.0008315667311350505, progress_reward 0, jerk_reward -0.0017904281616210938\n",
      "reward: -0.0009588614304860434\n",
      "reward, 196\n",
      "speed: 0.0008068666458129883, progress_reward 0, jerk_reward 6.210088729858399e-05\n",
      "reward: 0.0008689675331115723\n",
      "reward, 197\n",
      "speed: 0.0007599333127339681, progress_reward 0, jerk_reward 0.0022746872901916503\n",
      "reward: 0.0030346206029256183\n",
      "reward, 198\n",
      "speed: 0.0007469666004180908, progress_reward 0, jerk_reward -0.00038094282150268554\n",
      "reward: 0.0003660237789154053\n",
      "reward, 199\n",
      "speed: 0.0007287333806355794, progress_reward 0, jerk_reward -0.0005079364776611328\n",
      "reward: 0.00022079690297444662\n",
      "reward, 200\n",
      "speed: 0.000706000010172526, progress_reward 0, jerk_reward 0.00018784046173095703\n",
      "reward: 0.0008938404719034831\n",
      "reward, 201\n",
      "speed: 0.0007884333928426107, progress_reward 0, jerk_reward -0.0018366408348083497\n",
      "reward: -0.001048207441965739\n",
      "reward, 202\n",
      "speed: 0.0007926666736602783, progress_reward 0, jerk_reward 2.0372867584228516e-05\n",
      "reward: 0.0008130395412445068\n",
      "reward, 203\n",
      "speed: 0.0007754666805267334, progress_reward 0, jerk_reward 0.0015752410888671875\n",
      "reward: 0.002350707769393921\n",
      "reward, 204\n",
      "speed: 0.0007597333590189615, progress_reward 0, jerk_reward 7.472515106201172e-05\n",
      "reward: 0.0008344585100809733\n",
      "reward, 205\n",
      "speed: 0.0006717666784922282, progress_reward 0, jerk_reward 0.0018207669258117677\n",
      "reward: 0.0024925336043039956\n",
      "reward, 206\n",
      "speed: 0.0006326666673024496, progress_reward 0, jerk_reward -0.0008679354190826417\n",
      "reward: -0.0002352687517801921\n",
      "reward, 207\n",
      "speed: 0.0006069333553314209, progress_reward 0, jerk_reward -0.0006471884250640869\n",
      "reward: -4.025506973266604e-05\n",
      "reward, 208\n",
      "speed: 0.0007142333984375, progress_reward 0, jerk_reward -0.003610759973526001\n",
      "reward: -0.002896526575088501\n",
      "reward, 209\n",
      "speed: 0.0007503999869028727, progress_reward 0, jerk_reward 0.0014655935764312745\n",
      "reward: 0.002215993563334147\n",
      "reward, 210\n",
      "speed: 0.0008650666872660319, progress_reward 0, jerk_reward -0.0014892816543579102\n",
      "reward: -0.0006242149670918783\n",
      "reward, 211\n",
      "speed: 0.0009748667081197102, progress_reward 0, jerk_reward -0.0002491450309753418\n",
      "reward: 0.0007257216771443685\n",
      "reward, 212\n",
      "speed: 0.0010853333473205566, progress_reward 0, jerk_reward -4.3461322784423826e-05\n",
      "reward: 0.0010418720245361328\n",
      "reward, 213\n",
      "speed: 0.0012052000363667807, progress_reward 0, jerk_reward 1.3914108276367187e-05\n",
      "reward: 0.0012191141446431478\n",
      "reward, 214\n",
      "speed: 0.0013197666803995767, progress_reward 0, jerk_reward -0.0015021133422851562\n",
      "reward: -0.00018234666188557945\n",
      "reward, 215\n",
      "speed: 0.001331700086593628, progress_reward 0, jerk_reward 0.004042558670043946\n",
      "reward: 0.005374258756637574\n",
      "reward, 216\n",
      "speed: 0.0014001668294270834, progress_reward 0, jerk_reward -0.0006106877326965332\n",
      "reward: 0.0007894790967305503\n",
      "reward, 217\n",
      "speed: 0.0014646000862121582, progress_reward 0, jerk_reward -0.001585988998413086\n",
      "reward: -0.00012138891220092769\n",
      "reward, 218\n",
      "speed: 0.0013908333778381349, progress_reward 0, jerk_reward 0.004675397872924805\n",
      "reward: 0.00606623125076294\n",
      "reward, 219\n",
      "speed: 0.0013585333824157715, progress_reward 0, jerk_reward -0.0005455875396728516\n",
      "reward: 0.0008129458427429199\n",
      "reward, 220\n",
      "speed: 0.0013026667435963948, progress_reward 0, jerk_reward 0.0004795050621032715\n",
      "reward: 0.0017821718056996662\n",
      "reward, 221\n",
      "speed: 0.0012730000813802084, progress_reward 0, jerk_reward -0.0010171222686767578\n",
      "reward: 0.0002558778127034506\n",
      "reward, 222\n",
      "speed: 0.0012570000489552815, progress_reward 0, jerk_reward -0.00035896301269531253\n",
      "reward: 0.000898037036259969\n",
      "reward, 223\n",
      "speed: 0.0012414000034332276, progress_reward 0, jerk_reward 0.00018203496932983398\n",
      "reward: 0.0014234349727630616\n",
      "reward, 224\n",
      "speed: 0.0012228333950042726, progress_reward 0, jerk_reward -0.00014369964599609376\n",
      "reward: 0.0010791337490081787\n",
      "reward, 225\n",
      "speed: 0.0012088000774383544, progress_reward 0, jerk_reward -6.398677825927734e-05\n",
      "reward: 0.001144813299179077\n",
      "reward, 226\n",
      "speed: 0.001304466724395752, progress_reward 0, jerk_reward -0.001845693588256836\n",
      "reward: -0.0005412268638610841\n",
      "reward, 227\n",
      "speed: 0.001300000031789144, progress_reward 0, jerk_reward 0.00025716304779052737\n",
      "reward: 0.0015571630795796713\n",
      "reward, 228\n",
      "speed: 0.0012476000785827637, progress_reward 0, jerk_reward 0.0016913819313049317\n",
      "reward: 0.0029389820098876954\n",
      "reward, 229\n",
      "speed: 0.0012196667194366456, progress_reward 0, jerk_reward 0.0022943687438964845\n",
      "reward: 0.00351403546333313\n",
      "reward, 230\n",
      "speed: 0.0013323334058125815, progress_reward 0, jerk_reward -0.0059714674949646\n",
      "reward: -0.004639134089152019\n",
      "reward, 231\n",
      "speed: 0.0014660000801086427, progress_reward 0, jerk_reward -0.0016428351402282715\n",
      "reward: -0.00017683506011962886\n",
      "reward, 232\n",
      "speed: 0.0014923334121704102, progress_reward 0, jerk_reward 0.003692929744720459\n",
      "reward: 0.005185263156890869\n",
      "reward, 233\n",
      "speed: 0.001473166783650716, progress_reward 0, jerk_reward 0.0016350555419921875\n",
      "reward: 0.0031082223256429036\n",
      "reward, 234\n",
      "speed: 0.0013663333257039388, progress_reward 0, jerk_reward 0.0024626159667968752\n",
      "reward: 0.0038289492925008143\n",
      "reward, 235\n",
      "speed: 0.0012860000928243001, progress_reward 0, jerk_reward -0.000954892635345459\n",
      "reward: 0.00033110745747884113\n",
      "reward, 236\n",
      "speed: 0.0012146000862121582, progress_reward 0, jerk_reward 9.777545928955078e-06\n",
      "reward: 0.0012243776321411133\n",
      "reward, 237\n",
      "speed: 0.0011964667638142904, progress_reward 0, jerk_reward -0.0008166623115539551\n",
      "reward: 0.0003798044522603354\n",
      "reward, 238\n",
      "speed: 0.0011950000921885173, progress_reward 0, jerk_reward -0.001108708381652832\n",
      "reward: 8.629171053568525e-05\n",
      "reward, 239\n",
      "speed: 0.0011930667559305827, progress_reward 0, jerk_reward -5.574464797973633e-05\n",
      "reward: 0.0011373221079508463\n",
      "reward, 240\n",
      "speed: 0.0011921000480651855, progress_reward 0, jerk_reward -2.9447078704833986e-05\n",
      "reward: 0.0011626529693603514\n",
      "reward, 241\n",
      "speed: 0.001296333392461141, progress_reward 0, jerk_reward -0.0031664752960205078\n",
      "reward: -0.0018701419035593667\n",
      "reward, 242\n",
      "speed: 0.0013406333923339845, progress_reward 0, jerk_reward 0.0017958784103393555\n",
      "reward: 0.00313651180267334\n",
      "reward, 243\n",
      "speed: 0.0014076666831970214, progress_reward 0, jerk_reward -2.691030502319336e-05\n",
      "reward: 0.001380756378173828\n",
      "reward, 244\n",
      "speed: 0.0014851334889729817, progress_reward 0, jerk_reward -0.0017642641067504884\n",
      "reward: -0.0002791306177775067\n",
      "reward, 245\n",
      "speed: 0.0014683334032694498, progress_reward 0, jerk_reward 0.003827934265136719\n",
      "reward: 0.005296267668406168\n",
      "reward, 246\n",
      "speed: 0.0014479335149129233, progress_reward 0, jerk_reward -0.00017741680145263672\n",
      "reward: 0.0012705167134602865\n",
      "reward, 247\n",
      "speed: 0.0013442001342773438, progress_reward 0, jerk_reward 0.0023373460769653322\n",
      "reward: 0.003681546211242676\n",
      "reward, 248\n",
      "speed: 0.0012981667518615723, progress_reward 0, jerk_reward -0.0012249302864074708\n",
      "reward: 7.323646545410144e-05\n",
      "reward, 249\n",
      "speed: 0.00123853333791097, progress_reward 0, jerk_reward -0.0004056525230407715\n",
      "reward: 0.0008328808148701985\n",
      "reward, 250\n",
      "speed: 0.0012115000089009604, progress_reward 0, jerk_reward 2.6204586029052734e-05\n",
      "reward: 0.001237704594930013\n",
      "reward, 251\n",
      "speed: 0.001115399996439616, progress_reward 0, jerk_reward 0.0017923784255981445\n",
      "reward: 0.0029077784220377604\n",
      "reward, 252\n",
      "speed: 0.0010689000288645427, progress_reward 0, jerk_reward -0.001662154197692871\n",
      "reward: -0.0005932541688283284\n",
      "reward, 253\n",
      "speed: 0.001014833450317383, progress_reward 0, jerk_reward -4.827976226806641e-06\n",
      "reward: 0.0010100054740905763\n",
      "reward, 254\n",
      "speed: 0.0009906667073567709, progress_reward 0, jerk_reward -0.0005134320259094239\n",
      "reward: 0.000477234681447347\n",
      "reward, 255\n",
      "speed: 0.0009574000040690104, progress_reward 0, jerk_reward -0.0003016519546508789\n",
      "reward: 0.0006557480494181315\n",
      "reward, 256\n",
      "speed: 0.0009412666956583659, progress_reward 0, jerk_reward 0.0003265976905822754\n",
      "reward: 0.0012678643862406413\n",
      "reward, 257\n",
      "speed: 0.0009278667767842611, progress_reward 0, jerk_reward -0.00035208702087402344\n",
      "reward: 0.0005757797559102376\n",
      "reward, 258\n",
      "speed: 0.0009154000282287597, progress_reward 0, jerk_reward -0.0001907968521118164\n",
      "reward: 0.0007246031761169434\n",
      "reward, 259\n",
      "speed: 0.0009038333892822266, progress_reward 0, jerk_reward -8.516311645507812e-06\n",
      "reward: 0.0008953170776367189\n",
      "reward, 260\n",
      "speed: 0.0008908000787099203, progress_reward 0, jerk_reward 1.1355876922607422e-05\n",
      "reward: 0.0009021559556325277\n",
      "reward, 261\n",
      "speed: 0.0008473333517710368, progress_reward 0, jerk_reward 0.0010491871833801269\n",
      "reward: 0.0018965205351511637\n",
      "reward, 262\n",
      "speed: 0.0008308667341868082, progress_reward 0, jerk_reward -0.00045586585998535155\n",
      "reward: 0.0003750008742014567\n",
      "reward, 263\n",
      "speed: 0.0008580000400543213, progress_reward 0, jerk_reward -0.0022507023811340334\n",
      "reward: -0.0013927023410797122\n",
      "reward, 264\n",
      "speed: 0.0008466667334238688, progress_reward 0, jerk_reward 0.0016505742073059083\n",
      "reward: 0.002497240940729777\n",
      "reward, 265\n",
      "speed: 0.0008901666800181071, progress_reward 0, jerk_reward -5.332469940185547e-05\n",
      "reward: 0.0008368419806162517\n",
      "reward, 266\n",
      "speed: 0.0008802000681559245, progress_reward 0, jerk_reward -0.0015713882446289063\n",
      "reward: -0.0006911881764729818\n",
      "reward, 267\n",
      "speed: 0.0008633333841959635, progress_reward 0, jerk_reward 0.0017655110359191895\n",
      "reward: 0.002628844420115153\n",
      "reward, 268\n",
      "speed: 0.000852933406829834, progress_reward 0, jerk_reward -0.00014522552490234375\n",
      "reward: 0.0007077078819274903\n",
      "reward, 269\n",
      "speed: 0.0009448667367299398, progress_reward 0, jerk_reward -0.0017647147178649902\n",
      "reward: -0.0008198479811350504\n",
      "reward, 270\n",
      "speed: 0.000999000072479248, progress_reward 0, jerk_reward -0.0014188385009765625\n",
      "reward: -0.0004198384284973145\n",
      "reward, 271\n",
      "speed: 0.0009802667299906413, progress_reward 0, jerk_reward 0.00329911470413208\n",
      "reward: 0.004279381434122721\n",
      "reward, 272\n",
      "speed: 0.0009611666997273763, progress_reward 0, jerk_reward 0.00010146379470825195\n",
      "reward: 0.0010626304944356282\n",
      "reward, 273\n",
      "speed: 0.0009051334063212077, progress_reward 0, jerk_reward -7.217168807983398e-05\n",
      "reward: 0.0008329617182413737\n",
      "reward, 274\n",
      "speed: 0.0008642000357309977, progress_reward 0, jerk_reward 0.0019475865364074708\n",
      "reward: 0.0028117865721384687\n",
      "reward, 275\n",
      "speed: 0.0009552666346232096, progress_reward 0, jerk_reward -0.003768777847290039\n",
      "reward: -0.0028135112126668293\n",
      "reward, 276\n",
      "speed: 0.0009508334000905355, progress_reward 0, jerk_reward -1.850128173828125e-06\n",
      "reward: 0.0009489832719167073\n",
      "reward, 277\n",
      "speed: 0.000997599999109904, progress_reward 0, jerk_reward 0.00030567407608032225\n",
      "reward: 0.0013032740751902263\n",
      "reward, 278\n",
      "speed: 0.0009630334377288819, progress_reward 0, jerk_reward 0.001738896369934082\n",
      "reward: 0.002701929807662964\n",
      "reward, 279\n",
      "speed: 0.0009136000474294026, progress_reward 0, jerk_reward 0.0005608034133911132\n",
      "reward: 0.0014744034608205159\n",
      "reward, 280\n",
      "speed: 0.0007795333862304687, progress_reward 0, jerk_reward 0.0013932085037231445\n",
      "reward: 0.002172741889953613\n",
      "reward, 281\n",
      "speed: 0.0006986667315165202, progress_reward 0, jerk_reward 0.0015059208869934082\n",
      "reward: 0.002204587618509928\n",
      "reward, 282\n",
      "speed: 0.0006787333488464355, progress_reward 0, jerk_reward -0.003618006706237793\n",
      "reward: -0.0029392733573913575\n",
      "reward, 283\n",
      "speed: 0.0005794000625610351, progress_reward 0, jerk_reward 0.0017242980003356935\n",
      "reward: 0.0023036980628967285\n",
      "reward, 284\n",
      "speed: 0.0005383333365122478, progress_reward 0, jerk_reward -0.0003703463077545166\n",
      "reward: 0.00016798702875773114\n",
      "reward, 285\n",
      "speed: 0.0005176666975021362, progress_reward 0, jerk_reward -0.0011531245708465577\n",
      "reward: -0.0006354578733444215\n",
      "reward, 286\n",
      "speed: 0.0005068333546320598, progress_reward 0, jerk_reward -0.00048491358757019043\n",
      "reward: 2.1919767061869336e-05\n",
      "reward, 287\n",
      "speed: 0.0003994667132695516, progress_reward 0, jerk_reward 0.0017755353450775147\n",
      "reward: 0.0021750020583470664\n",
      "reward, 288\n",
      "speed: 0.0003637666702270508, progress_reward 0, jerk_reward -5.8209896087646486e-05\n",
      "reward: 0.0003055567741394043\n",
      "reward, 289\n",
      "speed: 0.00035006670157114667, progress_reward 0, jerk_reward -0.001319483518600464\n",
      "reward: -0.0009694168170293173\n",
      "reward, 290\n",
      "speed: 0.00023979999621709188, progress_reward 0, jerk_reward 0.001278742551803589\n",
      "reward: 0.0015185425480206808\n",
      "reward, 291\n",
      "speed: 0.00022540001074473063, progress_reward 0, jerk_reward -7.443904876708985e-05\n",
      "reward: 0.00015096096197764078\n",
      "reward, 292\n",
      "speed: 0.0002166666785875956, progress_reward 0, jerk_reward -0.0016285806894302368\n",
      "reward: -0.0014119140108426412\n",
      "reward, 293\n",
      "speed: 0.00020530001322428387, progress_reward 0, jerk_reward 0.00012685120105743408\n",
      "reward: 0.00033215121428171795\n",
      "reward, 294\n",
      "speed: 0.00010290000836054484, progress_reward 0, jerk_reward 0.001570846438407898\n",
      "reward: 0.0016737464467684428\n",
      "reward, 295\n",
      "speed: 7.446667551994323e-05, progress_reward 0, jerk_reward 3.9227306842803955e-06\n",
      "reward: 7.838940620422363e-05\n",
      "reward, 296\n",
      "speed: 4.080000271399816e-05, progress_reward 0, jerk_reward -0.0012923030555248261\n",
      "reward: -0.001251503052810828\n",
      "reward, 297\n",
      "speed: -0.0001308000087738037, progress_reward 0, jerk_reward -0.0016494105756282807\n",
      "reward: -0.0017802105844020844\n",
      "reward, 298\n",
      "speed: 2.0666675021251043e-06, progress_reward 0, jerk_reward 0.002644956111907959\n",
      "reward: 0.002647022779410084\n",
      "reward, 299\n",
      "speed: 6.833333522081375e-06, progress_reward 0, jerk_reward -0.0010523034632205963\n",
      "reward: -0.001045470129698515\n",
      "reward, 300\n",
      "speed: 4.000000034769376e-06, progress_reward 0, jerk_reward -0.0006906311213970184\n",
      "reward: -0.0006866311213622491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 33\u001b[0m\n\u001b[1;32m     17\u001b[0m     model \u001b[38;5;241m=\u001b[39m SAC(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     18\u001b[0m                 env, \n\u001b[1;32m     19\u001b[0m                 verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m                     net_arch\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m],\n\u001b[1;32m     30\u001b[0m                     optimizer_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/trackmania_sac7\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# cleanup\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/sac/sac.py:307\u001b[0m, in \u001b[0;36mSAC.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfSAC,\n\u001b[1;32m    300\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    306\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfSAC:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:328\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_freq, TrainFreq)  \u001b[38;5;66;03m# check done in _setup_learn()\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 328\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_noise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_noise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_starts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rollout\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[1;32m    339\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/off_policy_algorithm.py:560\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    557\u001b[0m actions, buffer_actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    563\u001b[0m num_collected_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ml/trackmania/venv/lib/python3.12/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[0;32mIn[6], line 89\u001b[0m, in \u001b[0;36mTrackmaniaEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_start_time         \u001b[38;5;66;03m# count step time\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 89\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcapture\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m                         \u001b[38;5;66;03m# capture new window\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtelemetry \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mretrieve_data()        \u001b[38;5;66;03m# get telemetry data# \u001b[39;00m\n\u001b[1;32m     92\u001b[0m truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_truncated(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtelemetry)   \u001b[38;5;66;03m# check if car has crashed\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mWindowCap.capture\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcapture\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrab\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonitor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize))       \u001b[38;5;66;03m# resize\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGRA2GRAY)        \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps = 1_000\n",
    "\n",
    "# creat env\n",
    "env = TrackmaniaEnv()\n",
    "env.reset()\n",
    "\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "load = False\n",
    "\n",
    "# load model\n",
    "if load:\n",
    "    model = SAC.load(\"models/trackmania_sac6\")\n",
    "    model.set_env(env)\n",
    "else:\n",
    "    # if not loading, new model\n",
    "    model = SAC('MlpPolicy', \n",
    "                env, \n",
    "                verbose=1,\n",
    "                gradient_steps=1,\n",
    "                ent_coef='auto',\n",
    "                target_entropy='auto',\n",
    "                gamma=0.99,\n",
    "                tau=0.005,\n",
    "        # reset counters for e\n",
    "                learning_starts=4000,\n",
    "                buffer_size=600_000,\n",
    "                policy_kwargs=dict(\n",
    "                    net_arch=[512, 512],\n",
    "                    optimizer_kwargs=dict(weight_decay=1e-5)))\n",
    "\n",
    "# train\n",
    "model.learn(total_timesteps=steps, callback=CustomWandbCallback())\n",
    "model.save(\"models/trackmania_sac7\")\n",
    "\n",
    "# cleanup\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\noptimal_positions = {}\\n\\n# map points\\nclass PlotPoints() :\\n    def __init__(self):\\n        super(PlotPoints, self).__init__()\\n        self.client = client.TMClient()\\n        self.gamepad = GamepadHandler()\\n        self.time_start = time.time()\\n        self.steps = 0\\n\\n    def reset(self):\\n        self.gamepad.reset()\\n        print(\"reset\")\\n        time.sleep(1.5)\\n        self.time_start = time.time()\\n\\n    def step(self):\\n        self.steps += 1\\n        self.telemetry = self.client.retrieve_data()  # Retrieve the telemetry data\\n        optimal_positions[self.steps] = self.telemetry[\\'position\\']\\n        # Process telemetry data to compute the reward and determine if the episode is done\\n        \\n        position = self.telemetry[\\'position\\']\\n\\n        finished = self.telemetry[\\'finished\\']\\n        time_step = self.time_start - time.time()\\n        \\n        return position, time_step, finished\\n\\n    def close(self):\\n        del self.window\\n        self.client.close()  # Close the TMClient connection\\n\\n# create new text file and add optimal positions coordinates to the file\\n# save each line as x, y, z float only\\nenv = PlotPoints()\\n\\nenv.reset()\\n\\n\\nwhile True:\\n    env.step()\\n    print(\\'start\\')\\n    if env.telemetry[\\'finished\\']:\\n        break\\n\\ndef save_optimal_positions(optimal_positions):\\n    with open(\\'models/optimal_positons_2.txt\\', \\'w\\') as f:\\n        for key in optimal_positions:\\n            f.write(f\"{optimal_positions[key]}\\n\")\\n\\nsave_optimal_positions(optimal_positions)\\n'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "optimal_positions = {}\n",
    "\n",
    "# map points\n",
    "class PlotPoints() :\n",
    "    def __init__(self):\n",
    "        super(PlotPoints, self).__init__()\n",
    "        self.client = client.TMClient()\n",
    "        self.gamepad = GamepadHandler()\n",
    "        self.time_start = time.time()\n",
    "        self.steps = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.gamepad.reset()\n",
    "        print(\"reset\")\n",
    "        time.sleep(1.5)\n",
    "        self.time_start = time.time()\n",
    "\n",
    "    def step(self):\n",
    "        self.steps += 1\n",
    "        self.telemetry = self.client.retrieve_data()  # Retrieve the telemetry data\n",
    "        optimal_positions[self.steps] = self.telemetry['position']\n",
    "        # Process telemetry data to compute the reward and determine if the episode is done\n",
    "        \n",
    "        position = self.telemetry['position']\n",
    "\n",
    "        finished = self.telemetry['finished']\n",
    "        time_step = self.time_start - time.time()\n",
    "        \n",
    "        return position, time_step, finished\n",
    "\n",
    "    def close(self):\n",
    "        del self.window\n",
    "        self.client.close()  # Close the TMClient connection\n",
    "\n",
    "# create new text file and add optimal positions coordinates to the file\n",
    "# save each line as x, y, z float only\n",
    "env = PlotPoints()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "\n",
    "while True:\n",
    "    env.step()\n",
    "    print('start')\n",
    "    if env.telemetry['finished']:\n",
    "        break\n",
    "\n",
    "def save_optimal_positions(optimal_positions):\n",
    "    with open('models/optimal_positons_2.txt', 'w') as f:\n",
    "        for key in optimal_positions:\n",
    "            f.write(f\"{optimal_positions[key]}\\n\")\n",
    "\n",
    "save_optimal_positions(optimal_positions)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pseudo checkpoints\n",
    "# drive around the track, save position every second\n",
    "# save the positions to a file\n",
    "# when driving count number of psedu checkpoints passed\n",
    "# by creating a bounding box around the checkpoint\n",
    "# only count checkpoint once\n",
    "# reward will then be checkpoints passed / time taken\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
